{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae55af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch \n",
    "from torch import optim \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif 'arm64' in platform.platform():\n",
    "    device = torch.device('mps') # 'mps'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'device: {device.type}') \n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# pytorch ignores this label in the loss\n",
    "ignore_index = -100\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# which transformer to use\n",
    "transformer_name = \"bert-base-cased\" # 'xlm-roberta-base' # 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4beaebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# map labels to the first token in each word\n",
    "def align_labels(word_ids, labels, label_to_index):\n",
    "    label_ids = []\n",
    "    previous_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None or word_id == previous_word_id:\n",
    "            # ignore if not a word or word id has already been seen\n",
    "            label_ids.append(ignore_index)\n",
    "        else:\n",
    "            # get label id for corresponding word\n",
    "            label_id = label_to_index[labels[word_id]]\n",
    "            label_ids.append(label_id)\n",
    "        # remember this word id\n",
    "        previous_word_id = word_id\n",
    "    \n",
    "    return label_ids\n",
    "            \n",
    "# build a set of labels in the dataset            \n",
    "def read_label_set(fn):\n",
    "    labels = set()\n",
    "    with open(fn) as f:\n",
    "        for index, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            if tokens != []:\n",
    "                label = tokens[-1]\n",
    "                labels.add(label)\n",
    "    return labels\n",
    "\n",
    "# converts a two-column file in the basic MTL format (\"word \\t label\") into a dataframe\n",
    "def read_dataframe(fn, label_to_index, task_id):\n",
    "    # now build the actual dataframe for this dataset\n",
    "    data = {'words': [], 'str_labels': [], 'input_ids': [], 'word_ids': [], 'labels': [], 'task_ids': []}\n",
    "    with open(fn) as f:\n",
    "        sent_words = []\n",
    "        sent_labels = [] \n",
    "        for index, line in tqdm(enumerate(f)):\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            if tokens == []:\n",
    "                data['words'].append(sent_words)\n",
    "                data['str_labels'].append(sent_labels)\n",
    "                \n",
    "                # tokenize each sentence\n",
    "                token_input = tokenizer(sent_words, is_split_into_words = True)  \n",
    "                token_ids = token_input['input_ids']\n",
    "                word_ids = token_input.word_ids(batch_index = 0)\n",
    "                \n",
    "                # map labels to the first token in each word\n",
    "                token_labels = align_labels(word_ids, sent_labels, label_to_index)\n",
    "                \n",
    "                data['input_ids'].append(token_ids)\n",
    "                data['word_ids'].append(word_ids)\n",
    "                data['labels'].append(token_labels)\n",
    "                data['task_ids'].append(task_id)\n",
    "                sent_words = []\n",
    "                sent_labels = [] \n",
    "            else:\n",
    "                sent_words.append(tokens[0])\n",
    "                sent_labels.append(tokens[1])\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b3e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task():\n",
    "    def __init__(self, task_id, train_file_name, dev_file_name, test_file_name):\n",
    "        self.task_id = task_id\n",
    "        # we need an index of labels first\n",
    "        self.labels = read_label_set(train_file_name)\n",
    "        self.index_to_label = {i:t for i,t in enumerate(self.labels)}\n",
    "        self.label_to_index = {t:i for i,t in enumerate(self.labels)}\n",
    "        self.num_labels = len(self.index_to_label)\n",
    "        # create data frames for the datasets\n",
    "        self.train_df = read_dataframe(train_file_name, self.label_to_index, self.task_id)\n",
    "        self.dev_df = read_dataframe(dev_file_name, self.label_to_index, self.task_id)\n",
    "        self.test_df = read_dataframe(test_file_name, self.label_to_index, self.task_id)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d43b086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0a0ca9c1cf43d7a6cea78f776e53e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f500cb800f0746d1884323be9a1bfe2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49da1191e31a42c2b94256c7a3e99721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c223083d67b44b588f26fa6c33f88dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef964ad4150401485679ad86f7763d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe927f808fa94fd69e535bd5b8e9ca5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_task = Task(0, \"data/conll-ner/train_small.txt\", \"data/conll-ner/dev.txt\", \"data/conll-ner/test.txt\")\n",
    "pos_task = Task(1, \"data/pos/train_small.txt\", \"data/pos/dev.txt\", \"data/pos/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90478984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>str_labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>task_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
       "      <td>[101, 7270, 22961, 1528, 1840, 1106, 21423, 14...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]</td>\n",
       "      <td>[-100, 8, 4, 2, 4, 4, 4, 2, 4, -100, 4, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td>[101, 1943, 14428, 102]</td>\n",
       "      <td>[None, 0, 1, None]</td>\n",
       "      <td>[-100, 7, 6, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "      <td>[101, 26660, 13329, 12649, 15928, 1820, 118, 4...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]</td>\n",
       "      <td>[-100, 5, -100, -100, -100, 4, -100, -100, -10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...</td>\n",
       "      <td>[101, 1109, 1735, 2827, 1163, 1113, 9170, 1122...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 4, 8, 0, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Germany, 's, representative, to, the, Europea...</td>\n",
       "      <td>[B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-P...</td>\n",
       "      <td>[101, 1860, 112, 188, 4702, 1106, 1103, 1735, ...</td>\n",
       "      <td>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 5, 4, -100, 4, 4, 4, 8, 0, 4, -100, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[\", We, do, n't, support, any, such, recommend...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[101, 107, 1284, 1202, 183, 112, 189, 1619, 12...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, -100, -100, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[He, said, further, scientific, study, was, re...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[101, 1124, 1163, 1748, 3812, 2025, 1108, 2320...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[He, said, a, proposal, last, month, by, EU, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-ORG, O, O, B-PER, I-PE...</td>\n",
       "      <td>[101, 1124, 1163, 170, 5835, 1314, 2370, 1118,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, 4, 4, 4, 8, 4, 4, 7, 6, -10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Fischler, proposed, EU-wide, measures, after,...</td>\n",
       "      <td>[B-PER, O, B-MISC, O, O, O, O, B-LOC, O, B-LOC...</td>\n",
       "      <td>[101, 17355, 9022, 2879, 3000, 7270, 118, 2043...</td>\n",
       "      <td>[None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, ...</td>\n",
       "      <td>[-100, 7, -100, -100, 4, 2, -100, -100, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[But, Fischler, agreed, to, review, his, propo...</td>\n",
       "      <td>[O, B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O...</td>\n",
       "      <td>[101, 1252, 17355, 9022, 2879, 2675, 1106, 318...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 4, 7, -100, -100, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Spanish, Farm, Minister, Loyola, de, Palacio,...</td>\n",
       "      <td>[B-MISC, O, O, B-PER, I-PER, I-PER, O, O, O, B...</td>\n",
       "      <td>[101, 2124, 6865, 2110, 23828, 1260, 19585, 17...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 9, ...</td>\n",
       "      <td>[-100, 2, 4, 4, 7, 6, 6, -100, -100, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[.]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[101, 119, 102]</td>\n",
       "      <td>[None, 0, None]</td>\n",
       "      <td>[-100, 4, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Only, France, and, Britain, backed, Fischler,...</td>\n",
       "      <td>[O, B-LOC, O, B-LOC, O, B-PER, O, O, O]</td>\n",
       "      <td>[101, 2809, 1699, 1105, 2855, 5534, 17355, 902...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 6, 7, 8, None]</td>\n",
       "      <td>[-100, 4, 5, 4, 5, 4, 7, -100, -100, 4, -100, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[The, EU, 's, scientific, veterinary, and, mul...</td>\n",
       "      <td>[O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[101, 1109, 7270, 112, 188, 3812, 27431, 1105,...</td>\n",
       "      <td>[None, 0, 1, 2, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 4, 8, 4, -100, 4, 4, 4, 4, -100, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[Sheep, have, long, been, known, to, contract,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-M...</td>\n",
       "      <td>[101, 1153, 8043, 1138, 1263, 1151, 1227, 1106...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 4, -100, 4, 4, 4, 4, 4, 4, 4, -100, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[British, farmers, denied, on, Thursday, there...</td>\n",
       "      <td>[B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[101, 1418, 6915, 5762, 1113, 9170, 1175, 1108...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[\", What, we, have, to, be, extremely, careful...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[101, 107, 1327, 1195, 1138, 1106, 1129, 4450,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[Bonn, has, led, efforts, to, protect, public,...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[101, 21066, 1144, 1521, 3268, 1106, 3244, 147...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[Germany, imported, 47,600, sheep, from, Brita...</td>\n",
       "      <td>[B-LOC, O, O, O, O, B-LOC, O, O, O, O, O, O, O...</td>\n",
       "      <td>[101, 1860, 11095, 3862, 117, 4372, 8892, 1121...</td>\n",
       "      <td>[None, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 5, 4, 4, -100, -100, 4, 4, 5, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[It, brought, in, 4,275, tonnes, of, British, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O...</td>\n",
       "      <td>[101, 1135, 1814, 1107, 125, 117, 18783, 10992...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 7, 7, 8, ...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, -100, -100, 4, 4, 2, 4, -10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[101, 118, 141, 9244, 9272, 12426, 1942, 118, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 0, None]</td>\n",
       "      <td>[-100, 4, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[Rare, Hendrix, song, draft, sells, for, almos...</td>\n",
       "      <td>[O, B-PER, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[101, 25574, 22609, 1461, 5039, 16695, 1111, 1...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 8, 9, None]</td>\n",
       "      <td>[-100, 4, 7, 4, 4, 4, 4, 4, 4, 4, -100, -100, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[LONDON, 1996-08-22]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "      <td>[101, 149, 11414, 2137, 11414, 1820, 118, 4775...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]</td>\n",
       "      <td>[-100, 5, -100, -100, -100, 4, -100, -100, -10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[A, rare, early, handwritten, draft, of, a, so...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-LOC, O, O, B-PER...</td>\n",
       "      <td>[101, 138, 4054, 1346, 1289, 16674, 5039, 1104...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 9, 9, ...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, -100, 4, 4, 4, 4, 4, 5, -10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[A, Florida, restaurant, paid, 10,925, pounds,...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[101, 138, 2631, 4382, 3004, 1275, 117, 5556, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 4, 4, 4, 5, 6, 7, 8, 8, ...</td>\n",
       "      <td>[-100, 4, 5, 4, 4, 4, -100, -100, -100, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[At, the, end, of, a, January, 1967, concert, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-MISC, O, O, B...</td>\n",
       "      <td>[101, 1335, 1103, 1322, 1104, 170, 1356, 2573,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[Buyers, also, snapped, up, 16, other, items, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-P...</td>\n",
       "      <td>[101, 26123, 1468, 1145, 5267, 1146, 1479, 116...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
       "      <td>[-100, 4, -100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[They, included, a, black, lacquer, and, mothe...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...</td>\n",
       "      <td>[101, 1220, 1529, 170, 1602, 2495, 1665, 19061...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 9, ...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, 4, -100, -100, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[The, guitarist, died, of, a, drugs, overdose,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[101, 1109, 5506, 1452, 1104, 170, 5557, 1166,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 6, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, 4, 4, 4, -100, -100, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[101, 118, 141, 9244, 9272, 12426, 1942, 118, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 0, None]</td>\n",
       "      <td>[-100, 4, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[China, says, Taiwan, spoils, atmosphere, for,...</td>\n",
       "      <td>[B-LOC, O, B-LOC, O, O, O, O, O]</td>\n",
       "      <td>[101, 1975, 1867, 6036, 188, 5674, 8825, 6814,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, None]</td>\n",
       "      <td>[-100, 5, 4, 5, 4, -100, -100, 4, 4, 4, 4, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[BEIJING, 1996-08-22]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "      <td>[101, 139, 27514, 4538, 15740, 1820, 118, 4775...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]</td>\n",
       "      <td>[-100, 5, -100, -100, -100, 4, -100, -100, -10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[China, on, Thursday, accused, Taipei, of, spo...</td>\n",
       "      <td>[B-LOC, O, O, O, B-LOC, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[101, 1975, 1113, 9170, 4806, 14512, 1104, 188...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 6, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 5, 4, 4, 4, 5, 4, 4, -100, -100, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[Speaking, only, hours, after, Chinese, state,...</td>\n",
       "      <td>[O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[101, 14072, 1178, 2005, 1170, 1922, 1352, 239...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[State, media, quoted, China, 's, top, negotia...</td>\n",
       "      <td>[O, O, O, B-LOC, O, O, O, O, B-LOC, O, B-PER, ...</td>\n",
       "      <td>[101, 1426, 2394, 9129, 1975, 112, 188, 1499, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 4, 5, 6, 6, 6, 7, 8, 9, ...</td>\n",
       "      <td>[-100, 4, 4, 4, 5, 4, -100, 4, 4, -100, -100, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[\", Now, is, the, time, for, the, two, sides, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[101, 107, 1986, 1110, 1103, 1159, 1111, 1103,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[that, is, to, end, the, state, of, hostility,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[101, 1115, 1110, 1106, 1322, 1103, 1352, 1104...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, -10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[The, foreign, ministry, 's, Shen, told, Reute...</td>\n",
       "      <td>[O, O, O, O, B-ORG, O, B-ORG, I-ORG, O, O, O, ...</td>\n",
       "      <td>[101, 1109, 2880, 8382, 112, 188, 26197, 1500,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, 4, 5, 6, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 4, 4, 4, 4, -100, 8, 4, 8, -100, 0, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[China, ,, which, has, long, opposed, all, Tai...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, B-LOC, O, O, O, O, O...</td>\n",
       "      <td>[101, 1975, 117, 1134, 1144, 1263, 4151, 1155,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[101, 118, 141, 9244, 9272, 12426, 1942, 118, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 0, None]</td>\n",
       "      <td>[-100, 4, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[China, says, time, right, for, Taiwan, talks, .]</td>\n",
       "      <td>[B-LOC, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[101, 1975, 1867, 1159, 1268, 1111, 6036, 7430...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, None]</td>\n",
       "      <td>[-100, 5, 4, 4, 4, 4, 5, 4, 4, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[BEIJING, 1996-08-22]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "      <td>[101, 139, 27514, 4538, 15740, 1820, 118, 4775...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]</td>\n",
       "      <td>[-100, 5, -100, -100, -100, 4, -100, -100, -10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[China, has, said, it, was, time, for, politic...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, O, O, O, B-LOC, O, O...</td>\n",
       "      <td>[101, 1975, 1144, 1163, 1122, 1108, 1159, 1111...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                words  \\\n",
       "0   [EU, rejects, German, call, to, boycott, Briti...   \n",
       "1                                  [Peter, Blackburn]   \n",
       "2                              [BRUSSELS, 1996-08-22]   \n",
       "3   [The, European, Commission, said, on, Thursday...   \n",
       "4   [Germany, 's, representative, to, the, Europea...   \n",
       "5   [\", We, do, n't, support, any, such, recommend...   \n",
       "6   [He, said, further, scientific, study, was, re...   \n",
       "7   [He, said, a, proposal, last, month, by, EU, F...   \n",
       "8   [Fischler, proposed, EU-wide, measures, after,...   \n",
       "9   [But, Fischler, agreed, to, review, his, propo...   \n",
       "10  [Spanish, Farm, Minister, Loyola, de, Palacio,...   \n",
       "11                                                [.]   \n",
       "12  [Only, France, and, Britain, backed, Fischler,...   \n",
       "13  [The, EU, 's, scientific, veterinary, and, mul...   \n",
       "14  [Sheep, have, long, been, known, to, contract,...   \n",
       "15  [British, farmers, denied, on, Thursday, there...   \n",
       "16  [\", What, we, have, to, be, extremely, careful...   \n",
       "17  [Bonn, has, led, efforts, to, protect, public,...   \n",
       "18  [Germany, imported, 47,600, sheep, from, Brita...   \n",
       "19  [It, brought, in, 4,275, tonnes, of, British, ...   \n",
       "20                                       [-DOCSTART-]   \n",
       "21  [Rare, Hendrix, song, draft, sells, for, almos...   \n",
       "22                               [LONDON, 1996-08-22]   \n",
       "23  [A, rare, early, handwritten, draft, of, a, so...   \n",
       "24  [A, Florida, restaurant, paid, 10,925, pounds,...   \n",
       "25  [At, the, end, of, a, January, 1967, concert, ...   \n",
       "26  [Buyers, also, snapped, up, 16, other, items, ...   \n",
       "27  [They, included, a, black, lacquer, and, mothe...   \n",
       "28  [The, guitarist, died, of, a, drugs, overdose,...   \n",
       "29                                       [-DOCSTART-]   \n",
       "30  [China, says, Taiwan, spoils, atmosphere, for,...   \n",
       "31                              [BEIJING, 1996-08-22]   \n",
       "32  [China, on, Thursday, accused, Taipei, of, spo...   \n",
       "33  [Speaking, only, hours, after, Chinese, state,...   \n",
       "34  [State, media, quoted, China, 's, top, negotia...   \n",
       "35  [\", Now, is, the, time, for, the, two, sides, ...   \n",
       "36  [that, is, to, end, the, state, of, hostility,...   \n",
       "37  [The, foreign, ministry, 's, Shen, told, Reute...   \n",
       "38  [China, ,, which, has, long, opposed, all, Tai...   \n",
       "39                                       [-DOCSTART-]   \n",
       "40  [China, says, time, right, for, Taiwan, talks, .]   \n",
       "41                              [BEIJING, 1996-08-22]   \n",
       "42  [China, has, said, it, was, time, for, politic...   \n",
       "\n",
       "                                           str_labels  \\\n",
       "0           [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]   \n",
       "1                                      [B-PER, I-PER]   \n",
       "2                                          [B-LOC, O]   \n",
       "3   [O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...   \n",
       "4   [B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-P...   \n",
       "5   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "6   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "7   [O, O, O, O, O, O, O, B-ORG, O, O, B-PER, I-PE...   \n",
       "8   [B-PER, O, B-MISC, O, O, O, O, B-LOC, O, B-LOC...   \n",
       "9   [O, B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O...   \n",
       "10  [B-MISC, O, O, B-PER, I-PER, I-PER, O, O, O, B...   \n",
       "11                                                [O]   \n",
       "12            [O, B-LOC, O, B-LOC, O, B-PER, O, O, O]   \n",
       "13  [O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "14  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-M...   \n",
       "15  [B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O...   \n",
       "16  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "17  [B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "18  [B-LOC, O, O, O, O, B-LOC, O, O, O, O, O, O, O...   \n",
       "19  [O, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O...   \n",
       "20                                                [O]   \n",
       "21                 [O, B-PER, O, O, O, O, O, O, O, O]   \n",
       "22                                         [B-LOC, O]   \n",
       "23  [O, O, O, O, O, O, O, O, O, B-LOC, O, O, B-PER...   \n",
       "24  [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "25  [O, O, O, O, O, O, O, O, O, O, B-MISC, O, O, B...   \n",
       "26  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-P...   \n",
       "27  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...   \n",
       "28               [O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "29                                                [O]   \n",
       "30                   [B-LOC, O, B-LOC, O, O, O, O, O]   \n",
       "31                                         [B-LOC, O]   \n",
       "32  [B-LOC, O, O, O, B-LOC, O, O, O, O, O, O, O, O...   \n",
       "33  [O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O...   \n",
       "34  [O, O, O, B-LOC, O, O, O, O, B-LOC, O, B-PER, ...   \n",
       "35      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "36  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "37  [O, O, O, O, B-ORG, O, B-ORG, I-ORG, O, O, O, ...   \n",
       "38  [B-LOC, O, O, O, O, O, O, B-LOC, O, O, O, O, O...   \n",
       "39                                                [O]   \n",
       "40                   [B-LOC, O, O, O, O, B-LOC, O, O]   \n",
       "41                                         [B-LOC, O]   \n",
       "42  [B-LOC, O, O, O, O, O, O, O, O, O, B-LOC, O, O...   \n",
       "\n",
       "                                            input_ids  \\\n",
       "0   [101, 7270, 22961, 1528, 1840, 1106, 21423, 14...   \n",
       "1                             [101, 1943, 14428, 102]   \n",
       "2   [101, 26660, 13329, 12649, 15928, 1820, 118, 4...   \n",
       "3   [101, 1109, 1735, 2827, 1163, 1113, 9170, 1122...   \n",
       "4   [101, 1860, 112, 188, 4702, 1106, 1103, 1735, ...   \n",
       "5   [101, 107, 1284, 1202, 183, 112, 189, 1619, 12...   \n",
       "6   [101, 1124, 1163, 1748, 3812, 2025, 1108, 2320...   \n",
       "7   [101, 1124, 1163, 170, 5835, 1314, 2370, 1118,...   \n",
       "8   [101, 17355, 9022, 2879, 3000, 7270, 118, 2043...   \n",
       "9   [101, 1252, 17355, 9022, 2879, 2675, 1106, 318...   \n",
       "10  [101, 2124, 6865, 2110, 23828, 1260, 19585, 17...   \n",
       "11                                    [101, 119, 102]   \n",
       "12  [101, 2809, 1699, 1105, 2855, 5534, 17355, 902...   \n",
       "13  [101, 1109, 7270, 112, 188, 3812, 27431, 1105,...   \n",
       "14  [101, 1153, 8043, 1138, 1263, 1151, 1227, 1106...   \n",
       "15  [101, 1418, 6915, 5762, 1113, 9170, 1175, 1108...   \n",
       "16  [101, 107, 1327, 1195, 1138, 1106, 1129, 4450,...   \n",
       "17  [101, 21066, 1144, 1521, 3268, 1106, 3244, 147...   \n",
       "18  [101, 1860, 11095, 3862, 117, 4372, 8892, 1121...   \n",
       "19  [101, 1135, 1814, 1107, 125, 117, 18783, 10992...   \n",
       "20  [101, 118, 141, 9244, 9272, 12426, 1942, 118, ...   \n",
       "21  [101, 25574, 22609, 1461, 5039, 16695, 1111, 1...   \n",
       "22  [101, 149, 11414, 2137, 11414, 1820, 118, 4775...   \n",
       "23  [101, 138, 4054, 1346, 1289, 16674, 5039, 1104...   \n",
       "24  [101, 138, 2631, 4382, 3004, 1275, 117, 5556, ...   \n",
       "25  [101, 1335, 1103, 1322, 1104, 170, 1356, 2573,...   \n",
       "26  [101, 26123, 1468, 1145, 5267, 1146, 1479, 116...   \n",
       "27  [101, 1220, 1529, 170, 1602, 2495, 1665, 19061...   \n",
       "28  [101, 1109, 5506, 1452, 1104, 170, 5557, 1166,...   \n",
       "29  [101, 118, 141, 9244, 9272, 12426, 1942, 118, ...   \n",
       "30  [101, 1975, 1867, 6036, 188, 5674, 8825, 6814,...   \n",
       "31  [101, 139, 27514, 4538, 15740, 1820, 118, 4775...   \n",
       "32  [101, 1975, 1113, 9170, 4806, 14512, 1104, 188...   \n",
       "33  [101, 14072, 1178, 2005, 1170, 1922, 1352, 239...   \n",
       "34  [101, 1426, 2394, 9129, 1975, 112, 188, 1499, ...   \n",
       "35  [101, 107, 1986, 1110, 1103, 1159, 1111, 1103,...   \n",
       "36  [101, 1115, 1110, 1106, 1322, 1103, 1352, 1104...   \n",
       "37  [101, 1109, 2880, 8382, 112, 188, 26197, 1500,...   \n",
       "38  [101, 1975, 117, 1134, 1144, 1263, 4151, 1155,...   \n",
       "39  [101, 118, 141, 9244, 9272, 12426, 1942, 118, ...   \n",
       "40  [101, 1975, 1867, 1159, 1268, 1111, 6036, 7430...   \n",
       "41  [101, 139, 27514, 4538, 15740, 1820, 118, 4775...   \n",
       "42  [101, 1975, 1144, 1163, 1122, 1108, 1159, 1111...   \n",
       "\n",
       "                                             word_ids  \\\n",
       "0          [None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]   \n",
       "1                                  [None, 0, 1, None]   \n",
       "2             [None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]   \n",
       "3   [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "4   [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10,...   \n",
       "5   [None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "6   [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "7   [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "8   [None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, ...   \n",
       "9   [None, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "10  [None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 9, ...   \n",
       "11                                    [None, 0, None]   \n",
       "12   [None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 6, 7, 8, None]   \n",
       "13  [None, 0, 1, 2, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10,...   \n",
       "14  [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10,...   \n",
       "15  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "16  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "17  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "18  [None, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "19  [None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 7, 7, 8, ...   \n",
       "20                  [None, 0, 0, 0, 0, 0, 0, 0, None]   \n",
       "21   [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 8, 9, None]   \n",
       "22            [None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]   \n",
       "23  [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 9, 9, ...   \n",
       "24  [None, 0, 1, 2, 3, 4, 4, 4, 4, 5, 6, 7, 8, 8, ...   \n",
       "25  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "26  [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...   \n",
       "27  [None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 9, ...   \n",
       "28  [None, 0, 1, 2, 3, 4, 5, 6, 6, 6, 7, 8, 9, 10,...   \n",
       "29                  [None, 0, 0, 0, 0, 0, 0, 0, None]   \n",
       "30         [None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, None]   \n",
       "31            [None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]   \n",
       "32  [None, 0, 1, 2, 3, 4, 5, 6, 6, 6, 7, 8, 9, 10,...   \n",
       "33  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "34  [None, 0, 1, 2, 3, 4, 4, 5, 6, 6, 6, 7, 8, 9, ...   \n",
       "35  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "36  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "37  [None, 0, 1, 2, 3, 3, 4, 5, 6, 6, 7, 8, 9, 10,...   \n",
       "38  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "39                  [None, 0, 0, 0, 0, 0, 0, 0, None]   \n",
       "40               [None, 0, 1, 2, 3, 4, 5, 6, 7, None]   \n",
       "41            [None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]   \n",
       "42  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "\n",
       "                                               labels  task_ids  \n",
       "0       [-100, 8, 4, 2, 4, 4, 4, 2, 4, -100, 4, -100]         0  \n",
       "1                                  [-100, 7, 6, -100]         0  \n",
       "2   [-100, 5, -100, -100, -100, 4, -100, -100, -10...         0  \n",
       "3   [-100, 4, 8, 0, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, ...         0  \n",
       "4   [-100, 5, 4, -100, 4, 4, 4, 8, 0, 4, -100, 4, ...         0  \n",
       "5   [-100, 4, 4, 4, 4, -100, -100, 4, 4, 4, 4, 4, ...         0  \n",
       "6   [-100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...         0  \n",
       "7   [-100, 4, 4, 4, 4, 4, 4, 4, 8, 4, 4, 7, 6, -10...         0  \n",
       "8   [-100, 7, -100, -100, 4, 2, -100, -100, 4, 4, ...         0  \n",
       "9   [-100, 4, 7, -100, -100, 4, 4, 4, 4, 4, 4, 4, ...         0  \n",
       "10  [-100, 2, 4, 4, 7, 6, 6, -100, -100, 4, 4, 4, ...         0  \n",
       "11                                    [-100, 4, -100]         0  \n",
       "12  [-100, 4, 5, 4, 5, 4, 7, -100, -100, 4, -100, ...         0  \n",
       "13  [-100, 4, 8, 4, -100, 4, 4, 4, 4, -100, 4, 4, ...         0  \n",
       "14  [-100, 4, -100, 4, 4, 4, 4, 4, 4, 4, -100, 4, ...         0  \n",
       "15  [-100, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...         0  \n",
       "16  [-100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...         0  \n",
       "17  [-100, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...         0  \n",
       "18  [-100, 5, 4, 4, -100, -100, 4, 4, 5, 4, 4, 4, ...         0  \n",
       "19  [-100, 4, 4, 4, 4, -100, -100, 4, 4, 2, 4, -10...         0  \n",
       "20  [-100, 4, -100, -100, -100, -100, -100, -100, ...         0  \n",
       "21  [-100, 4, 7, 4, 4, 4, 4, 4, 4, 4, -100, -100, ...         0  \n",
       "22  [-100, 5, -100, -100, -100, 4, -100, -100, -10...         0  \n",
       "23  [-100, 4, 4, 4, 4, -100, 4, 4, 4, 4, 4, 5, -10...         0  \n",
       "24  [-100, 4, 5, 4, 4, 4, -100, -100, -100, 4, 4, ...         0  \n",
       "25  [-100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, ...         0  \n",
       "26  [-100, 4, -100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...         0  \n",
       "27  [-100, 4, 4, 4, 4, 4, -100, -100, 4, 4, 4, 4, ...         0  \n",
       "28  [-100, 4, 4, 4, 4, 4, 4, 4, -100, -100, 4, 4, ...         0  \n",
       "29  [-100, 4, -100, -100, -100, -100, -100, -100, ...         0  \n",
       "30   [-100, 5, 4, 5, 4, -100, -100, 4, 4, 4, 4, -100]         0  \n",
       "31  [-100, 5, -100, -100, -100, 4, -100, -100, -10...         0  \n",
       "32  [-100, 5, 4, 4, 4, 5, 4, 4, -100, -100, 4, 4, ...         0  \n",
       "33  [-100, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, ...         0  \n",
       "34  [-100, 4, 4, 4, 5, 4, -100, 4, 4, -100, -100, ...         0  \n",
       "35  [-100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...         0  \n",
       "36  [-100, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, -10...         0  \n",
       "37  [-100, 4, 4, 4, 4, -100, 8, 4, 8, -100, 0, 4, ...         0  \n",
       "38  [-100, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, ...         0  \n",
       "39  [-100, 4, -100, -100, -100, -100, -100, -100, ...         0  \n",
       "40               [-100, 5, 4, 4, 4, 4, 5, 4, 4, -100]         0  \n",
       "41  [-100, 5, -100, -100, -100, 4, -100, -100, -10...         0  \n",
       "42  [-100, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, ...         0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_task.train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a0c793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>str_labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>task_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "      <td>[101, 4855, 25354, 6378, 117, 5391, 1201, 1385...</td>\n",
       "      <td>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
       "      <td>[-100, 44, 44, -100, 17, 12, 33, 30, 17, 22, 4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "      <td>[101, 1828, 119, 25354, 6378, 1110, 3931, 1104...</td>\n",
       "      <td>[None, 0, 0, 1, 1, 2, 3, 4, 5, 5, 5, 6, 6, 6, ...</td>\n",
       "      <td>[-100, 44, -100, 44, -100, 10, 21, 18, 44, -10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "      <td>[101, 19922, 138, 8376, 2246, 117, 3731, 1201,...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 44, 44, -100, -100, 17, 12, 33, 30, 42,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "      <td>[101, 138, 1532, 1104, 1112, 12866, 11990, 151...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 16, 21, 18, 21, -100, -100, 20, 19, 6, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
       "      <td>[101, 1109, 1112, 12866, 11990, 12753, 117, 17...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, ...</td>\n",
       "      <td>[-100, 16, 21, -100, -100, 21, 17, 21, -100, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[On, the, other, hand, ,, had, it, existed, th...</td>\n",
       "      <td>[IN, DT, JJ, NN, ,, VBD, PRP, VBN, RB, ,, NNP,...</td>\n",
       "      <td>[101, 1212, 1103, 1168, 1289, 117, 1125, 1122,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1...</td>\n",
       "      <td>[-100, 18, 16, 30, 21, 17, 41, 9, 19, 20, 17, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[Mr., Cray, ,, who, could, n't, be, reached, f...</td>\n",
       "      <td>[NNP, NNP, ,, WP, MD, RB, VB, VBN, IN, NN, ,, ...</td>\n",
       "      <td>[101, 1828, 119, 140, 6447, 117, 1150, 1180, 1...</td>\n",
       "      <td>[None, 0, 0, 1, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, ...</td>\n",
       "      <td>[-100, 44, -100, 44, -100, 17, 39, 22, 20, -10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[Regarded, as, the, father, of, the, supercomp...</td>\n",
       "      <td>[VBN, IN, DT, NN, IN, DT, NN, ,, NNP, NNP, VBD...</td>\n",
       "      <td>[101, 23287, 26541, 1112, 1103, 1401, 1104, 11...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 6, 6, 6, 7, 8, ...</td>\n",
       "      <td>[-100, 19, -100, 18, 16, 21, 18, 16, 21, -100,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[At, Cray, Computer, ,, he, will, be, paid, $,...</td>\n",
       "      <td>[IN, NNP, NNP, ,, PRP, MD, VB, VBN, $, CD, .]</td>\n",
       "      <td>[101, 1335, 140, 6447, 6701, 117, 1119, 1209, ...</td>\n",
       "      <td>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, ...</td>\n",
       "      <td>[-100, 18, 44, -100, 44, 17, 9, 22, 43, 19, 28...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>[(, ), blah, merde, uh, pdt, #, ls, @, .]</td>\n",
       "      <td>[-LRB-, -RRB-, RBS, FW, UH, PDT, #, LS, SYM, .]</td>\n",
       "      <td>[101, 113, 114, 171, 10358, 1143, 14407, 14863...</td>\n",
       "      <td>[None, 0, 1, 2, 2, 3, 3, 4, 5, 5, 6, 7, 7, 8, ...</td>\n",
       "      <td>[-100, 14, 29, 11, -100, 27, -100, 38, 8, -100...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 words  \\\n",
       "0    [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1    [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
       "2    [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "3    [A, form, of, asbestos, once, used, to, make, ...   \n",
       "4    [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "..                                                 ...   \n",
       "196  [On, the, other, hand, ,, had, it, existed, th...   \n",
       "197  [Mr., Cray, ,, who, could, n't, be, reached, f...   \n",
       "198  [Regarded, as, the, father, of, the, supercomp...   \n",
       "199  [At, Cray, Computer, ,, he, will, be, paid, $,...   \n",
       "200          [(, ), blah, merde, uh, pdt, #, ls, @, .]   \n",
       "\n",
       "                                            str_labels  \\\n",
       "0    [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...   \n",
       "1    [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...   \n",
       "2    [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...   \n",
       "3    [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...   \n",
       "4    [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...   \n",
       "..                                                 ...   \n",
       "196  [IN, DT, JJ, NN, ,, VBD, PRP, VBN, RB, ,, NNP,...   \n",
       "197  [NNP, NNP, ,, WP, MD, RB, VB, VBN, IN, NN, ,, ...   \n",
       "198  [VBN, IN, DT, NN, IN, DT, NN, ,, NNP, NNP, VBD...   \n",
       "199      [IN, NNP, NNP, ,, PRP, MD, VB, VBN, $, CD, .]   \n",
       "200    [-LRB-, -RRB-, RBS, FW, UH, PDT, #, LS, SYM, .]   \n",
       "\n",
       "                                             input_ids  \\\n",
       "0    [101, 4855, 25354, 6378, 117, 5391, 1201, 1385...   \n",
       "1    [101, 1828, 119, 25354, 6378, 1110, 3931, 1104...   \n",
       "2    [101, 19922, 138, 8376, 2246, 117, 3731, 1201,...   \n",
       "3    [101, 138, 1532, 1104, 1112, 12866, 11990, 151...   \n",
       "4    [101, 1109, 1112, 12866, 11990, 12753, 117, 17...   \n",
       "..                                                 ...   \n",
       "196  [101, 1212, 1103, 1168, 1289, 117, 1125, 1122,...   \n",
       "197  [101, 1828, 119, 140, 6447, 117, 1150, 1180, 1...   \n",
       "198  [101, 23287, 26541, 1112, 1103, 1401, 1104, 11...   \n",
       "199  [101, 1335, 140, 6447, 6701, 117, 1119, 1209, ...   \n",
       "200  [101, 113, 114, 171, 10358, 1143, 14407, 14863...   \n",
       "\n",
       "                                              word_ids  \\\n",
       "0    [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...   \n",
       "1    [None, 0, 0, 1, 1, 2, 3, 4, 5, 5, 5, 6, 6, 6, ...   \n",
       "2    [None, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "3    [None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "4    [None, 0, 1, 1, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, ...   \n",
       "..                                                 ...   \n",
       "196  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1...   \n",
       "197  [None, 0, 0, 1, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, ...   \n",
       "198  [None, 0, 0, 1, 2, 3, 4, 5, 6, 6, 6, 6, 7, 8, ...   \n",
       "199  [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, ...   \n",
       "200  [None, 0, 1, 2, 2, 3, 3, 4, 5, 5, 6, 7, 7, 8, ...   \n",
       "\n",
       "                                                labels  task_ids  \n",
       "0    [-100, 44, 44, -100, 17, 12, 33, 30, 17, 22, 4...         1  \n",
       "1    [-100, 44, -100, 44, -100, 10, 21, 18, 44, -10...         1  \n",
       "2    [-100, 44, 44, -100, -100, 17, 12, 33, 30, 42,...         1  \n",
       "3    [-100, 16, 21, 18, 21, -100, -100, 20, 19, 6, ...         1  \n",
       "4    [-100, 16, 21, -100, -100, 21, 17, 21, -100, -...         1  \n",
       "..                                                 ...       ...  \n",
       "196  [-100, 18, 16, 30, 21, 17, 41, 9, 19, 20, 17, ...         1  \n",
       "197  [-100, 44, -100, 44, -100, 17, 39, 22, 20, -10...         1  \n",
       "198  [-100, 19, -100, 18, 16, 21, 18, 16, 21, -100,...         1  \n",
       "199  [-100, 18, 44, -100, 44, 17, 9, 22, 43, 19, 28...         1  \n",
       "200  [-100, 14, 29, 11, -100, 27, -100, 38, 8, -100...         1  \n",
       "\n",
       "[201 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_task.train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798f2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import BertModel, BertPreTrainedModel\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "# This class is adapted from: https://towardsdatascience.com/how-to-create-and-train-a-multi-task-transformer-model-18c54a146240\n",
    "class TokenClassificationModel(BertPreTrainedModel):    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.output_heads = nn.ModuleDict() # these are initialized in add_heads\n",
    "        self.init_weights()\n",
    "        \n",
    "    def add_heads(self, tasks):\n",
    "        for task in tasks:\n",
    "            head = TokenClassificationHead(self.bert.config.hidden_size, task.num_labels, config.hidden_dropout_prob)\n",
    "            # ModuleDict requires keys to be strings\n",
    "            self.output_heads[str(task.task_id)] = head\n",
    "        return self\n",
    "    \n",
    "    def summarize_heads(self):\n",
    "        print(f'Found {len(self.output_heads)} heads')\n",
    "        for task_id in self.output_heads:\n",
    "            self.output_heads[task_id].summarize(task_id)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, task_ids=None, **kwargs):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            **kwargs,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        #print(f'batch size = {len(input_ids)}')\n",
    "        #print(f'task_ids in this batch: {task_ids}')\n",
    "        \n",
    "        # generate specific predictions and losses for each task head\n",
    "        unique_task_ids_list = torch.unique(task_ids).tolist()\n",
    "        logits = None\n",
    "        loss_list = []\n",
    "        for unique_task_id in unique_task_ids_list:\n",
    "            task_id_filter = task_ids == unique_task_id\n",
    "            filtered_sequence_output = sequence_output[task_id_filter]\n",
    "            filtered_labels = None if labels is None else labels[task_id_filter]\n",
    "            filtered_attention_mask = None if attention_mask is None else attention_mask[task_id_filter]\n",
    "            #print(f'size of batch for task {unique_task_id} is: {len(filtered_sequence_output)}')\n",
    "            logits, task_loss = self.output_heads[str(unique_task_id)].forward(\n",
    "                filtered_sequence_output, None,\n",
    "                filtered_labels,\n",
    "                filtered_attention_mask,\n",
    "            )\n",
    "            if filtered_labels is not None:\n",
    "                loss_list.append(task_loss)\n",
    "                \n",
    "        loss = None if len(loss_list) == 0 else torch.stack(loss_list)\n",
    "                    \n",
    "        # logits are only used for eval, in which case we handle a single task at a time\n",
    "        # TODO: allow all tasks in the forward pass at inference                     \n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss.mean(),\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "class TokenClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if self.classifier.bias is not None:\n",
    "            self.classifier.bias.data.zero_()\n",
    "            \n",
    "    def summarize(self, task_id):\n",
    "        print(f\"Task {task_id} with {self.num_labels} labels.\")\n",
    "        print(f'Dropout is {self.dropout}')\n",
    "        print(f'Classifier layer is {self.classifier}')\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output, labels=None, attention_mask=None, **kwargs):\n",
    "        sequence_output_dropout = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output_dropout)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()            \n",
    "            inputs = logits.view(-1, self.num_labels)\n",
    "            targets = labels.view(-1)\n",
    "            loss = loss_fn(inputs, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9945a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing TokenClassificationModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TokenClassificationModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TokenClassificationModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 heads\n",
      "Task 0 with 9 labels.\n",
      "Dropout is Dropout(p=0.1, inplace=False)\n",
      "Classifier layer is Linear(in_features=768, out_features=9, bias=True)\n",
      "Task 1 with 45 labels.\n",
      "Dropout is Dropout(p=0.1, inplace=False)\n",
      "Classifier layer is Linear(in_features=768, out_features=45, bias=True)\n"
     ]
    }
   ],
   "source": [
    "tasks = [ner_task, pos_task]\n",
    "config = AutoConfig.from_pretrained(transformer_name)\n",
    "model= TokenClassificationModel.from_pretrained(transformer_name, config=config).add_heads(tasks)\n",
    "model.summarize_heads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c45587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # gold labels\n",
    "    label_ids = eval_pred.label_ids\n",
    "    # predictions\n",
    "    pred_ids = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    # collect gold and predicted labels, ignoring ignore_index label\n",
    "    y_true, y_pred = [], []\n",
    "    batch_size, seq_len = pred_ids.shape\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != ignore_index:\n",
    "                y_true.append(label_ids[i][j]) #index_to_label[label_ids[i][j]])\n",
    "                y_pred.append(pred_ids[i][j]) #index_to_label[pred_ids[i][j]])\n",
    "    # return computed metrics\n",
    "    return {'accuracy': accuracy_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49167b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['words', 'str_labels', 'input_ids', 'word_ids', 'labels', 'task_ids', '__index_level_0__'],\n",
       "        num_rows: 244\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['words', 'str_labels', 'input_ids', 'word_ids', 'labels', 'task_ids', '__index_level_0__'],\n",
       "        num_rows: 8504\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['words', 'str_labels', 'input_ids', 'word_ids', 'labels', 'task_ids', '__index_level_0__'],\n",
       "        num_rows: 6101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds['train'] = Dataset.from_pandas(pd.concat([ner_task.train_df, pos_task.train_df]))\n",
    "ds['validation'] = Dataset.from_pandas(pd.concat([ner_task.dev_df, pos_task.dev_df]))\n",
    "ds['test'] = Dataset.from_pandas(pd.concat([ner_task.test_df, pos_task.test_df]))\n",
    "\n",
    "# these are no longer needed; discard them to save memory\n",
    "ner_task.train_df = None\n",
    "pos_task.train_df = None\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48554618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# compute accuracy\n",
    "def evaluation_classification_report(trainer, task, name, useTest=False):\n",
    "    print(f\"Test classification report for task {name}:\")\n",
    "    num_labels = task.num_labels\n",
    "    df = task.test_df if useTest == False else task.dev_df\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    output = trainer.predict(ds)\n",
    "    label_ids = output.label_ids.reshape(-1)\n",
    "    predictions = output.predictions.reshape(-1, num_labels)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    mask = label_ids != ignore_index\n",
    "    \n",
    "    y_true = label_ids[mask]\n",
    "    y_pred = predictions[mask]\n",
    "    target_names = [task.index_to_label.get(ele, \"\") for ele in range(num_labels)]\n",
    "    print(target_names)\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for(t, p) in zip(y_true, y_pred):\n",
    "        total = total + 1\n",
    "        if t == p:\n",
    "            correct = correct + 1\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=target_names\n",
    "    )\n",
    "    print(report)\n",
    "    print(f'locally computed accuracy: {accuracy}')\n",
    "    return accuracy\n",
    "\n",
    "# compute loss and accuracy\n",
    "def evaluate(trainer, task, name):\n",
    "    print(f\"Evaluating on the validation dataset for task {name}:\")\n",
    "    ds = Dataset.from_pandas(task.dev_df)\n",
    "    scores = trainer.evaluate(ds)\n",
    "    acc = evaluation_classification_report(trainer, task, name, useTest = False)\n",
    "    return scores, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6756a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_task(task_head, task, task_name, task_checkpoint):\n",
    "    numpy_weights = task_head.classifier.weight.cpu().detach().numpy()\n",
    "    numpy_bias = task_head.classifier.bias.cpu().detach().numpy()\n",
    "    labels = task.labels\n",
    "    #print(f\"Shape of weights: {numpy_weights.shape}\")\n",
    "    #print(f\"Weights are:\\n{numpy_weights}\")\n",
    "    #print(f\"Shape of bias: {numpy_bias.shape}\")\n",
    "    #print(f\"Bias is: {numpy_bias}\")\n",
    "    #print(f\"Labels are: {labels}\")\n",
    "    \n",
    "    os.makedirs(task_checkpoint, exist_ok = True)\n",
    "    lf = open(task_checkpoint + \"/labels\", \"w\")\n",
    "    for label in labels:\n",
    "        lf.write(f'{label}\\n')\n",
    "    lf.close()\n",
    "    \n",
    "    wf = open(task_checkpoint + \"/weights\", \"w\")\n",
    "    wf.write(f'{numpy_weights.shape[0]} {numpy_weights.shape[1]}\\n')\n",
    "    for i, x in enumerate(numpy_weights):\n",
    "        for j, y in enumerate(x):\n",
    "            wf.write(f'{y} ')\n",
    "        wf.write('\\n')\n",
    "    wf.close()\n",
    "    \n",
    "    bf = open(task_checkpoint + \"/biases\", \"w\")\n",
    "    bf.write(f'{numpy_bias.shape[0]}\\n')\n",
    "    for i, x in enumerate(numpy_bias):\n",
    "        bf.write(f'{x} ')\n",
    "    bf.write('\\n')\n",
    "    bf.close()\n",
    "\n",
    "def onnx_save(model, checkpoint):\n",
    "    orig_words = [\"Using\", \"transformers\", \"with\", \"ONNX\", \"runtime\"]\n",
    "    token_input = tokenizer(orig_words, is_split_into_words = True, return_tensors = \"pt\")\n",
    "    print(token_input)\n",
    "    token_ids = token_input['input_ids']\n",
    "                \n",
    "    inputs = (token_ids) \n",
    "    input_names = [\"token_ids\"] \n",
    "    output_names = [\"sequence_output\"]\n",
    "\n",
    "    torch.onnx.export(model.bert,\n",
    "        inputs,\n",
    "        checkpoint,\n",
    "        export_params=True,\n",
    "        do_constant_folding=True,\n",
    "        input_names = input_names,\n",
    "        output_names = output_names,\n",
    "        opset_version=10, \n",
    "        dynamic_axes = {\"token_ids\": {1: 'sent length'}}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd912e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING EPOCH 1\n",
      "Found 2 heads\n",
      "Task 0 with 9 labels.\n",
      "Dropout is Dropout(p=0.1, inplace=False)\n",
      "Classifier layer is Linear(in_features=768, out_features=9, bias=True)\n",
      "Task 1 with 45 labels.\n",
      "Dropout is Dropout(p=0.1, inplace=False)\n",
      "Classifier layer is Linear(in_features=768, out_features=45, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/functional.py:769: UserWarning: The operator 'aten::_unique2' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1663830697895/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  output, inverse_indices, counts = torch._unique2(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for epoch 1: 0:00:03.097341\n",
      "Evaluating on the validation dataset for task NER:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification report for task NER:\n",
      "['I-ORG', 'I-LOC', 'B-MISC', 'I-MISC', 'O', 'B-LOC', 'I-PER', 'B-PER', 'B-ORG']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-ORG       0.05      0.01      0.02       835\n",
      "       I-LOC       0.00      0.00      0.00       257\n",
      "      B-MISC       0.00      0.00      0.00       702\n",
      "      I-MISC       0.00      0.00      0.00       216\n",
      "           O       0.82      0.95      0.88     38554\n",
      "       B-LOC       0.02      0.01      0.01      1668\n",
      "       I-PER       0.40      0.00      0.00      1156\n",
      "       B-PER       0.07      0.00      0.00      1617\n",
      "       B-ORG       0.03      0.00      0.00      1661\n",
      "\n",
      "    accuracy                           0.79     46666\n",
      "   macro avg       0.16      0.11      0.10     46666\n",
      "weighted avg       0.70      0.79      0.73     46666\n",
      "\n",
      "locally computed accuracy: 0.7857326533236189\n",
      "Evaluating on the validation dataset for task POS:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification report for task POS:\n",
      "['VBG', 'WDT', 'WP$', \"''\", '``', '#', 'TO', 'NNPS', 'PDT', 'PRP', 'VBZ', 'RBS', 'CD', '.', '-LRB-', 'EX', 'DT', ',', 'IN', 'VBN', 'RB', 'NN', 'MD', 'VBP', 'JJS', 'WRB', 'PRP$', 'FW', '$', '-RRB-', 'JJ', 'SYM', 'RBR', 'NNS', 'POS', 'JJR', ':', 'RP', 'UH', 'WP', 'LS', 'VBD', 'CC', 'VB', 'NNP']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         VBG       0.00      0.00      0.00       817\n",
      "         WDT       0.10      0.00      0.01       280\n",
      "         WP$       0.00      0.00      0.00        21\n",
      "          ''       0.02      0.01      0.01       512\n",
      "          ``       0.01      0.01      0.01       531\n",
      "           #       0.00      0.00      0.00         5\n",
      "          TO       0.00      0.00      0.00      1240\n",
      "        NNPS       0.00      0.00      0.00        42\n",
      "         PDT       0.00      0.00      0.00        18\n",
      "         PRP       0.01      0.00      0.00      1055\n",
      "         VBZ       0.01      0.01      0.01      1233\n",
      "         RBS       0.00      0.00      0.00        30\n",
      "          CD       0.02      0.00      0.00      1924\n",
      "           .       0.04      0.00      0.00      2363\n",
      "       -LRB-       0.00      0.00      0.00        72\n",
      "          EX       0.00      0.00      0.00        57\n",
      "          DT       0.11      0.02      0.03      4816\n",
      "           ,       0.12      0.00      0.00      3061\n",
      "          IN       0.16      0.06      0.08      5854\n",
      "         VBN       0.00      0.00      0.00      1099\n",
      "          RB       0.00      0.00      0.00      2053\n",
      "          NN       0.16      0.42      0.24      7568\n",
      "          MD       0.03      0.01      0.01       584\n",
      "         VBP       0.00      0.00      0.00       802\n",
      "         JJS       0.00      0.00      0.00       127\n",
      "         WRB       0.00      0.00      0.00       133\n",
      "        PRP$       0.00      0.00      0.00       509\n",
      "          FW       0.00      0.00      0.00        20\n",
      "           $       0.00      0.00      0.00       375\n",
      "       -RRB-       0.00      0.00      0.00        72\n",
      "          JJ       0.00      0.00      0.00      3707\n",
      "         SYM       0.00      0.00      0.00         1\n",
      "         RBR       0.00      0.02      0.01       112\n",
      "         NNS       0.05      0.26      0.08      3507\n",
      "         POS       0.04      0.00      0.01       549\n",
      "         JJR       0.00      0.00      0.00       198\n",
      "           :       0.12      0.01      0.02       241\n",
      "          RP       0.00      0.00      0.00       182\n",
      "          UH       0.00      0.00      0.00         8\n",
      "          WP       0.02      0.01      0.01       111\n",
      "          LS       0.00      0.00      0.00         4\n",
      "         VBD       0.00      0.00      0.00      1836\n",
      "          CC       0.03      0.01      0.01      1373\n",
      "          VB       0.00      0.00      0.00      1580\n",
      "         NNP       0.06      0.05      0.05      5918\n",
      "\n",
      "    accuracy                           0.09     56600\n",
      "   macro avg       0.02      0.02      0.01     56600\n",
      "weighted avg       0.07      0.09      0.05     56600\n",
      "\n",
      "locally computed accuracy: 0.08627208480565371\n",
      "DEV MACRO LOSS FOR EPOCH 1: 0.764058992266655\n",
      "\n",
      "\n",
      "DEV MACRO ACC FOR EPOCH 1: 0.4360023690646363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7993, 11303,  1468,  1114, 21748,  2249,  3190,  1576,  4974,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# save the ONNX model\u001b[39;00m\n\u001b[1;32m     65\u001b[0m onnx_checkpoint \u001b[38;5;241m=\u001b[39m training_args\u001b[38;5;241m.\u001b[39moutput_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/onnx_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch)\n\u001b[0;32m---> 66\u001b[0m \u001b[43monnx_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36monnx_save\u001b[0;34m(model, checkpoint)\u001b[0m\n\u001b[1;32m     41\u001b[0m input_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \n\u001b[1;32m     42\u001b[0m output_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msent length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/onnx/utils.py:483\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    190\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     export_modules_as_functions: Union[\u001b[38;5;28mbool\u001b[39m, Collection[Type[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m        model to the file ``f`` even if this is raised.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/onnx/utils.py:1469\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1467\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1469\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1484\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1485\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/onnx/utils.py:1081\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1078\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m   1080\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1081\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/onnx/utils.py:957\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    952\u001b[0m     graph \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m    953\u001b[0m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     )\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 957\u001b[0m graph, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    959\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/onnx/utils.py:860\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# When weights are not reused, there is no perf impact\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# ONNX runtimes can also apply CSE optimization to compensate the lack of cache here\u001b[39;00m\n\u001b[1;32m    859\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 860\u001b[0m trace_graph, torch_out, inputs_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    865\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/jit/_trace.py:1184\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1183\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m-> 1184\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mONNXTracedModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1015\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:239\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    236\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    242\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/nn/functional.py:2206\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2201\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2202\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "weight_decay = 0.01\n",
    "use_mps_device = True if str(device) == 'mps' else False\n",
    "model_name = f'{transformer_name}-mtl'\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "last_checkpoint = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f'STARTING EPOCH {epoch}')\n",
    "    if last_checkpoint != None:\n",
    "        print(f'Resuming from checkpoint {last_checkpoint}')\n",
    "            \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_name,\n",
    "        log_level='error',\n",
    "        num_train_epochs=1, # one epoch at a time\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        # evaluation_strategy='epoch',\n",
    "        do_eval=False, # we will evaluate each task explicitly\n",
    "        weight_decay=weight_decay,\n",
    "        resume_from_checkpoint = last_checkpoint,\n",
    "        use_mps_device = use_mps_device\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        # compute_metrics=compute_metrics,\n",
    "        train_dataset=ds['train'],\n",
    "        # eval_dataset=ds['validation'],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    model.summarize_heads()\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "    trainer.train()\n",
    "    end_time = time.monotonic()\n",
    "    print(f\"Elapsed time for epoch {epoch}: {timedelta(seconds=end_time - start_time)}\")\n",
    "\n",
    "    ner_scores, ner_acc = evaluate(trainer, ner_task, \"NER\")\n",
    "    pos_scores, pos_acc = evaluate(trainer, pos_task, \"POS\")\n",
    "    macro_loss = (ner_scores['eval_loss'] + pos_scores['eval_loss'])/2\n",
    "    print(f'DEV MACRO LOSS FOR EPOCH {epoch}: {macro_loss}\\n\\n')\n",
    "    macro_acc = (ner_acc + pos_acc)/2\n",
    "    print(f'DEV MACRO ACC FOR EPOCH {epoch}: {macro_acc}')\n",
    "\n",
    "    # save the transformer encoder + the head for each task\n",
    "    last_checkpoint = training_args.output_dir + '/mtl_model_epoch' + str(epoch)\n",
    "    trainer.save_model(last_checkpoint)\n",
    "    save_task(model.output_heads[\"0\"], ner_task, \"NER\", last_checkpoint + \"/ner_head\")\n",
    "    save_task(model.output_heads[\"1\"], pos_task, \"POS\", last_checkpoint + \"/pos_head\")\n",
    "    \n",
    "    # save the ONNX model\n",
    "    onnx_checkpoint = training_args.output_dir + '/onnx_epoch' + str(epoch)\n",
    "    onnx_save(model, onnx_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba0ddaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = TokenClassificationModel.from_pretrained('bert-base-cased-mtl/mtl_model_epoch2', local_files_only=True)\n",
    "#model.summarize_heads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "864a1bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification report for task NER:\n",
      "['I-MISC', 'B-MISC', 'O', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'B-ORG', 'I-LOC']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      I-MISC       0.00      0.00      0.00       346\n",
      "      B-MISC       0.31      0.53      0.39       922\n",
      "           O       0.97      0.97      0.97     42975\n",
      "       B-PER       0.58      0.59      0.59      1842\n",
      "       I-PER       0.70      0.50      0.58      1307\n",
      "       B-LOC       0.53      0.76      0.63      1837\n",
      "       I-ORG       0.32      0.33      0.32       751\n",
      "       B-ORG       0.27      0.16      0.20      1341\n",
      "       I-LOC       0.00      0.00      0.00       257\n",
      "\n",
      "    accuracy                           0.88     51578\n",
      "   macro avg       0.41      0.43      0.41     51578\n",
      "weighted avg       0.89      0.88      0.88     51578\n",
      "\n",
      "locally computed accuracy: 0.8836131684051339\n",
      "Test classification report for task POS:\n",
      "['VBN', 'POS', 'IN', 'VBZ', 'PRP', 'WP', '.', 'VB', 'WRB', '-LRB-', 'DT', \"''\", '``', 'JJS', ',', 'RBS', '#', 'TO', 'UH', ':', '$', 'NN', 'RBR', 'NNS', 'EX', 'VBG', 'NNP', 'VBD', 'CD', 'SYM', 'JJR', 'LS', 'JJ', 'PDT', 'RP', 'CC', 'WDT', '-RRB-', 'MD', 'PRP$', 'NNPS', 'VBP', 'WP$', 'FW', 'RB']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         VBN       0.74      0.79      0.76      2729\n",
      "         POS       0.95      0.99      0.97      1153\n",
      "          IN       0.89      0.98      0.93     12639\n",
      "         VBZ       0.96      0.95      0.95      2434\n",
      "         PRP       0.97      0.97      0.97      1932\n",
      "          WP       0.00      0.00      0.00       278\n",
      "           .       1.00      0.97      0.98      4974\n",
      "          VB       0.87      0.95      0.91      3367\n",
      "         WRB       0.00      0.00      0.00       258\n",
      "       -LRB-       0.00      0.00      0.00       182\n",
      "          DT       0.96      0.98      0.97     10542\n",
      "          ''       0.97      1.00      0.99       834\n",
      "          ``       1.00      0.31      0.48       852\n",
      "         JJS       0.00      0.00      0.00       232\n",
      "           ,       0.94      1.00      0.97      6173\n",
      "         RBS       0.00      0.00      0.00        55\n",
      "           #       0.00      0.00      0.00        25\n",
      "          TO       1.00      0.99      0.99      2836\n",
      "          UH       0.00      0.00      0.00        11\n",
      "           :       1.00      0.41      0.58       706\n",
      "           $       1.00      0.98      0.99       956\n",
      "          NN       0.89      0.93      0.91     17300\n",
      "         RBR       0.00      0.00      0.00       210\n",
      "         NNS       0.98      0.90      0.94      7634\n",
      "          EX       0.00      0.00      0.00       109\n",
      "         VBG       0.98      0.59      0.74      1836\n",
      "         NNP       0.92      0.97      0.94     11563\n",
      "         VBD       0.97      0.88      0.92      4250\n",
      "          CD       0.96      0.97      0.96      5031\n",
      "         SYM       0.00      0.00      0.00        10\n",
      "         JJR       0.00      0.00      0.00       451\n",
      "          LS       0.00      0.00      0.00        14\n",
      "          JJ       0.67      0.79      0.73      7394\n",
      "         PDT       0.00      0.00      0.00        43\n",
      "          RP       0.00      0.00      0.00       297\n",
      "          CC       0.99      0.89      0.94      3016\n",
      "         WDT       0.68      0.68      0.68       550\n",
      "       -RRB-       0.00      0.00      0.00       186\n",
      "          MD       0.99      0.94      0.97      1204\n",
      "        PRP$       1.00      0.94      0.97       920\n",
      "        NNPS       0.00      0.00      0.00       278\n",
      "         VBP       0.98      0.63      0.77      1365\n",
      "         WP$       0.00      0.00      0.00        24\n",
      "          FW       0.00      0.00      0.00        12\n",
      "          RB       0.61      0.80      0.69      3708\n",
      "\n",
      "    accuracy                           0.90    120573\n",
      "   macro avg       0.55      0.52      0.52    120573\n",
      "weighted avg       0.89      0.90      0.89    120573\n",
      "\n",
      "locally computed accuracy: 0.8971743259270317\n",
      "MTL macro accuracy: 0.8903937471660828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "ner_acc = evaluation_classification_report(trainer, ner_task, \"NER\", useTest = True)\n",
    "pos_acc = evaluation_classification_report(trainer, pos_task, \"POS\", useTest = True)\n",
    "macro_acc = (ner_acc + pos_acc)/2\n",
    "print(f\"MTL macro accuracy: {macro_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24770d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
