{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif 'arm64' in platform.platform():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# which transformer to use\n",
    "transformer_name = 'distilbert-base-cased' # 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beaebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# map labels to the first token in each word\n",
    "def align_labels(word_ids, labels, label_to_index):\n",
    "    # default value for CrossEntropyLoss ignore_index parameter\n",
    "    ignore_index = -100\n",
    "    \n",
    "    label_ids = []\n",
    "    previous_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None or word_id == previous_word_id:\n",
    "            # ignore if not a word or word id has already been seen\n",
    "            label_ids.append(ignore_index)\n",
    "        else:\n",
    "            # get label id for corresponding word\n",
    "            label_id = label_to_index[labels[word_id]]\n",
    "            label_ids.append(label_id)\n",
    "        # remember this word id\n",
    "        previous_word_id = word_id\n",
    "    \n",
    "    return label_ids\n",
    "            \n",
    "# build a set of labels in the dataset            \n",
    "def read_label_set(fn):\n",
    "    labels = set()\n",
    "    with open(fn) as f:\n",
    "        for index, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            if tokens != []:\n",
    "                label = tokens[-1]\n",
    "                labels.add(label)\n",
    "    return labels\n",
    "\n",
    "# converts a two-column file in the basic MTL format (\"word \\t label\") into a dataframe\n",
    "def read_dataframe(fn):\n",
    "    \n",
    "    labels = read_label_set(fn)\n",
    "    index_to_label = {i:t for i,t in enumerate(labels)}\n",
    "    label_to_index = {t:i for i,t in enumerate(labels)}\n",
    "    print(\"index_to_label: \", index_to_label)\n",
    "    \n",
    "    # now build the actual dataframe for this dataset\n",
    "    data = {'words': [], 'labels': [], 'token_ids': [], 'word_ids': [], 'token_labels': []}\n",
    "    with open(fn) as f:\n",
    "        sent_words = []\n",
    "        sent_labels = [] \n",
    "        for index, line in tqdm(enumerate(f)):\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            if tokens == []:\n",
    "                data['words'].append(sent_words)\n",
    "                data['labels'].append(sent_labels)\n",
    "                \n",
    "                # tokenize each sentence\n",
    "                token_input = tokenizer(sent_words, is_split_into_words = True)  \n",
    "                token_ids = token_input['input_ids']\n",
    "                word_ids = token_input.word_ids(batch_index = 0)\n",
    "                \n",
    "                # map labels to the first token in each word\n",
    "                token_labels = align_labels(word_ids, sent_labels, label_to_index)\n",
    "                \n",
    "                data['token_ids'].append(token_ids)\n",
    "                data['word_ids'].append(word_ids)\n",
    "                data['token_labels'].append(token_labels)\n",
    "                sent_words = []\n",
    "                sent_labels = [] \n",
    "            else:\n",
    "                sent_words.append(tokens[0])\n",
    "                sent_labels.append(tokens[1])\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_dataframe(\"data/conll-ner/train_small.txt\")\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43b086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
