{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae55af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch \n",
    "from torch import optim \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif 'arm64' in platform.platform():\n",
    "    device = torch.device('mps') # 'mps'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'device: {device.type}') \n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# pytorch ignores this label in the loss\n",
    "ignore_index = -100\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# which transformer to use\n",
    "transformer_name = \"bert-base-cased\" # 'xlm-roberta-base' # 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4beaebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# map labels to the first token in each word\n",
    "def align_labels(word_ids, labels, label_to_index):\n",
    "    label_ids = []\n",
    "    previous_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None or word_id == previous_word_id:\n",
    "            # ignore if not a word or word id has already been seen\n",
    "            label_ids.append(ignore_index)\n",
    "        else:\n",
    "            # get label id for corresponding word\n",
    "            label_id = label_to_index[labels[word_id]]\n",
    "            label_ids.append(label_id)\n",
    "        # remember this word id\n",
    "        previous_word_id = word_id\n",
    "    \n",
    "    return label_ids\n",
    "            \n",
    "# build a set of labels in the dataset            \n",
    "def read_label_set(fn):\n",
    "    labels = set()\n",
    "    with open(fn) as f:\n",
    "        for index, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            if tokens != []:\n",
    "                label = tokens[-1]\n",
    "                labels.add(label)\n",
    "    return labels\n",
    "\n",
    "# converts a two-column file in the basic MTL format (\"word \\t label\") into a dataframe\n",
    "def read_dataframe(fn, label_to_index, task_id):\n",
    "    # now build the actual dataframe for this dataset\n",
    "    data = {'words': [], 'str_labels': [], 'input_ids': [], 'word_ids': [], 'labels': [], 'task_ids': []}\n",
    "    with open(fn) as f:\n",
    "        sent_words = []\n",
    "        sent_labels = [] \n",
    "        for index, line in tqdm(enumerate(f)):\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            if tokens == []:\n",
    "                data['words'].append(sent_words)\n",
    "                data['str_labels'].append(sent_labels)\n",
    "                \n",
    "                # tokenize each sentence\n",
    "                token_input = tokenizer(sent_words, is_split_into_words = True)  \n",
    "                token_ids = token_input['input_ids']\n",
    "                word_ids = token_input.word_ids(batch_index = 0)\n",
    "                \n",
    "                # map labels to the first token in each word\n",
    "                token_labels = align_labels(word_ids, sent_labels, label_to_index)\n",
    "                \n",
    "                data['input_ids'].append(token_ids)\n",
    "                data['word_ids'].append(word_ids)\n",
    "                data['labels'].append(token_labels)\n",
    "                data['task_ids'].append(task_id)\n",
    "                sent_words = []\n",
    "                sent_labels = [] \n",
    "            else:\n",
    "                sent_words.append(tokens[0])\n",
    "                sent_labels.append(tokens[1])\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5b3e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task():\n",
    "    def __init__(self, task_id, train_file_name, dev_file_name, test_file_name):\n",
    "        self.task_id = task_id\n",
    "        # we need an index of labels first\n",
    "        self.labels = read_label_set(train_file_name)\n",
    "        self.index_to_label = {i:t for i,t in enumerate(self.labels)}\n",
    "        self.label_to_index = {t:i for i,t in enumerate(self.labels)}\n",
    "        self.num_labels = len(self.index_to_label)\n",
    "        # create data frames for the datasets\n",
    "        self.train_df = read_dataframe(train_file_name, self.label_to_index, self.task_id)\n",
    "        self.dev_df = read_dataframe(dev_file_name, self.label_to_index, self.task_id)\n",
    "        self.test_df = read_dataframe(test_file_name, self.label_to_index, self.task_id)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2d43b086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600ce00cdff948ecaa713021c80674e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d49b3239d748498f29b35ad1f637c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8af370bc3a4ae6b8fba4c12b8cde20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c0c05740784a38b64ef72c2a61ec20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fa453328e14e3ebb7dc8f2e45bd061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df523f34cfd347bdba75f60bf0e9acf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_task = Task(0, \"data/conll-ner/train.txt\", \"data/conll-ner/dev.txt\", \"data/conll-ner/test.txt\")\n",
    "pos_task = Task(1, \"data/pos/train.txt\", \"data/pos/dev.txt\", \"data/pos/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90478984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>str_labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>task_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[101, 118, 141, 9244, 9272, 12426, 1942, 118, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 0, None]</td>\n",
       "      <td>[-100, 4, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
       "      <td>[101, 7270, 22961, 1528, 1840, 1106, 21423, 14...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]</td>\n",
       "      <td>[-100, 1, 4, 8, 4, 4, 4, 8, 4, -100, 4, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td>[101, 1943, 14428, 102]</td>\n",
       "      <td>[None, 0, 1, None]</td>\n",
       "      <td>[-100, 3, 0, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "      <td>[101, 26660, 13329, 12649, 15928, 1820, 118, 4...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]</td>\n",
       "      <td>[-100, 6, -100, -100, -100, 4, -100, -100, -10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...</td>\n",
       "      <td>[101, 1109, 1735, 2827, 1163, 1113, 9170, 1122...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 4, 1, 7, 4, 4, 4, 4, 4, 4, 8, 4, 4, 4, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14982</th>\n",
       "      <td>[on, Friday, :]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[101, 1113, 5286, 131, 102]</td>\n",
       "      <td>[None, 0, 1, 2, None]</td>\n",
       "      <td>[-100, 4, 4, 4, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14983</th>\n",
       "      <td>[Division, two]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[101, 1784, 1160, 102]</td>\n",
       "      <td>[None, 0, 1, None]</td>\n",
       "      <td>[-100, 4, 4, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14984</th>\n",
       "      <td>[Plymouth, 2, Preston, 1]</td>\n",
       "      <td>[B-ORG, O, B-ORG, O]</td>\n",
       "      <td>[101, 10033, 123, 8083, 122, 102]</td>\n",
       "      <td>[None, 0, 1, 2, 3, None]</td>\n",
       "      <td>[-100, 1, 4, 1, 4, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>[Division, three]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[101, 1784, 1210, 102]</td>\n",
       "      <td>[None, 0, 1, None]</td>\n",
       "      <td>[-100, 4, 4, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14986</th>\n",
       "      <td>[Swansea, 1, Lincoln, 2]</td>\n",
       "      <td>[B-ORG, O, B-ORG, O]</td>\n",
       "      <td>[101, 17057, 122, 4617, 123, 102]</td>\n",
       "      <td>[None, 0, 1, 2, 3, None]</td>\n",
       "      <td>[-100, 1, 4, 1, 4, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14987 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   words  \\\n",
       "0                                           [-DOCSTART-]   \n",
       "1      [EU, rejects, German, call, to, boycott, Briti...   \n",
       "2                                     [Peter, Blackburn]   \n",
       "3                                 [BRUSSELS, 1996-08-22]   \n",
       "4      [The, European, Commission, said, on, Thursday...   \n",
       "...                                                  ...   \n",
       "14982                                    [on, Friday, :]   \n",
       "14983                                    [Division, two]   \n",
       "14984                          [Plymouth, 2, Preston, 1]   \n",
       "14985                                  [Division, three]   \n",
       "14986                           [Swansea, 1, Lincoln, 2]   \n",
       "\n",
       "                                              str_labels  \\\n",
       "0                                                    [O]   \n",
       "1              [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]   \n",
       "2                                         [B-PER, I-PER]   \n",
       "3                                             [B-LOC, O]   \n",
       "4      [O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...   \n",
       "...                                                  ...   \n",
       "14982                                          [O, O, O]   \n",
       "14983                                             [O, O]   \n",
       "14984                               [B-ORG, O, B-ORG, O]   \n",
       "14985                                             [O, O]   \n",
       "14986                               [B-ORG, O, B-ORG, O]   \n",
       "\n",
       "                                               input_ids  \\\n",
       "0      [101, 118, 141, 9244, 9272, 12426, 1942, 118, ...   \n",
       "1      [101, 7270, 22961, 1528, 1840, 1106, 21423, 14...   \n",
       "2                                [101, 1943, 14428, 102]   \n",
       "3      [101, 26660, 13329, 12649, 15928, 1820, 118, 4...   \n",
       "4      [101, 1109, 1735, 2827, 1163, 1113, 9170, 1122...   \n",
       "...                                                  ...   \n",
       "14982                        [101, 1113, 5286, 131, 102]   \n",
       "14983                             [101, 1784, 1160, 102]   \n",
       "14984                  [101, 10033, 123, 8083, 122, 102]   \n",
       "14985                             [101, 1784, 1210, 102]   \n",
       "14986                  [101, 17057, 122, 4617, 123, 102]   \n",
       "\n",
       "                                                word_ids  \\\n",
       "0                      [None, 0, 0, 0, 0, 0, 0, 0, None]   \n",
       "1             [None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]   \n",
       "2                                     [None, 0, 1, None]   \n",
       "3                [None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]   \n",
       "4      [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "...                                                  ...   \n",
       "14982                              [None, 0, 1, 2, None]   \n",
       "14983                                 [None, 0, 1, None]   \n",
       "14984                           [None, 0, 1, 2, 3, None]   \n",
       "14985                                 [None, 0, 1, None]   \n",
       "14986                           [None, 0, 1, 2, 3, None]   \n",
       "\n",
       "                                                  labels  task_ids  \n",
       "0      [-100, 4, -100, -100, -100, -100, -100, -100, ...         0  \n",
       "1          [-100, 1, 4, 8, 4, 4, 4, 8, 4, -100, 4, -100]         0  \n",
       "2                                     [-100, 3, 0, -100]         0  \n",
       "3      [-100, 6, -100, -100, -100, 4, -100, -100, -10...         0  \n",
       "4      [-100, 4, 1, 7, 4, 4, 4, 4, 4, 4, 8, 4, 4, 4, ...         0  \n",
       "...                                                  ...       ...  \n",
       "14982                              [-100, 4, 4, 4, -100]         0  \n",
       "14983                                 [-100, 4, 4, -100]         0  \n",
       "14984                           [-100, 1, 4, 1, 4, -100]         0  \n",
       "14985                                 [-100, 4, 4, -100]         0  \n",
       "14986                           [-100, 1, 4, 1, 4, -100]         0  \n",
       "\n",
       "[14987 rows x 6 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_task.train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a4a0c793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>str_labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>task_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "      <td>[101, 4855, 25354, 6378, 117, 5391, 1201, 1385...</td>\n",
       "      <td>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
       "      <td>[-100, 9, 9, -100, 39, 21, 0, 30, 39, 20, 10, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "      <td>[101, 1828, 119, 25354, 6378, 1110, 3931, 1104...</td>\n",
       "      <td>[None, 0, 0, 1, 1, 2, 3, 4, 5, 5, 5, 6, 6, 6, ...</td>\n",
       "      <td>[-100, 9, -100, 9, -100, 33, 37, 5, 9, -100, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "      <td>[101, 19922, 138, 8376, 2246, 117, 3731, 1201,...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 9, 9, -100, -100, 39, 21, 0, 30, 19, 30...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "      <td>[101, 138, 1532, 1104, 1112, 12866, 11990, 151...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 28, 37, 5, 37, -100, -100, 17, 26, 2, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
       "      <td>[101, 1109, 1112, 12866, 11990, 12753, 117, 17...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, ...</td>\n",
       "      <td>[-100, 28, 37, -100, -100, 37, 39, 37, -100, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46787</th>\n",
       "      <td>[Says, Peter, Mokaba, ,, president, of, the, S...</td>\n",
       "      <td>[VBZ, NNP, NNP, ,, NN, IN, DT, NNP, NNP, NNP, ...</td>\n",
       "      <td>[101, 8652, 1116, 1943, 12556, 1968, 2822, 117...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, ...</td>\n",
       "      <td>[-100, 33, -100, 9, 9, -100, -100, 39, 37, 5, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46788</th>\n",
       "      <td>[They, never, considered, themselves, to, be, ...</td>\n",
       "      <td>[PRP, RB, VBD, PRP, TO, VB, NN, RB, .]</td>\n",
       "      <td>[101, 1220, 1309, 1737, 2310, 1106, 1129, 1625...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]</td>\n",
       "      <td>[-100, 43, 17, 27, 43, 2, 10, 37, 17, 18, -100]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46789</th>\n",
       "      <td>[At, last, night, 's, rally, ,, they, called, ...</td>\n",
       "      <td>[IN, JJ, NN, POS, NN, ,, PRP, VBD, IN, PRP$, N...</td>\n",
       "      <td>[101, 1335, 1314, 1480, 112, 188, 11158, 117, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
       "      <td>[-100, 5, 30, 37, 23, -100, 37, 39, 43, 27, 5,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46790</th>\n",
       "      <td>[``, We, emphasize, discipline, because, we, k...</td>\n",
       "      <td>[``, PRP, VBP, NN, IN, PRP, VBP, IN, DT, NN, V...</td>\n",
       "      <td>[101, 169, 169, 1284, 19291, 9360, 1272, 1195,...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
       "      <td>[-100, 38, -100, 43, 16, 37, 5, 43, 16, 5, 28,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46791</th>\n",
       "      <td>[``, We, want, to, see, Nelson, Mandela, and, ...</td>\n",
       "      <td>[``, PRP, VBP, TO, VB, NNP, NNP, CC, PDT, PRP$...</td>\n",
       "      <td>[101, 169, 169, 1284, 1328, 1106, 1267, 5232, ...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 38, -100, 43, 16, 2, 10, 9, 9, -100, 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46792 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   words  \\\n",
       "0      [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1      [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
       "2      [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "3      [A, form, of, asbestos, once, used, to, make, ...   \n",
       "4      [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "...                                                  ...   \n",
       "46787  [Says, Peter, Mokaba, ,, president, of, the, S...   \n",
       "46788  [They, never, considered, themselves, to, be, ...   \n",
       "46789  [At, last, night, 's, rally, ,, they, called, ...   \n",
       "46790  [``, We, emphasize, discipline, because, we, k...   \n",
       "46791  [``, We, want, to, see, Nelson, Mandela, and, ...   \n",
       "\n",
       "                                              str_labels  \\\n",
       "0      [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...   \n",
       "1      [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...   \n",
       "2      [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...   \n",
       "3      [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...   \n",
       "4      [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...   \n",
       "...                                                  ...   \n",
       "46787  [VBZ, NNP, NNP, ,, NN, IN, DT, NNP, NNP, NNP, ...   \n",
       "46788             [PRP, RB, VBD, PRP, TO, VB, NN, RB, .]   \n",
       "46789  [IN, JJ, NN, POS, NN, ,, PRP, VBD, IN, PRP$, N...   \n",
       "46790  [``, PRP, VBP, NN, IN, PRP, VBP, IN, DT, NN, V...   \n",
       "46791  [``, PRP, VBP, TO, VB, NNP, NNP, CC, PDT, PRP$...   \n",
       "\n",
       "                                               input_ids  \\\n",
       "0      [101, 4855, 25354, 6378, 117, 5391, 1201, 1385...   \n",
       "1      [101, 1828, 119, 25354, 6378, 1110, 3931, 1104...   \n",
       "2      [101, 19922, 138, 8376, 2246, 117, 3731, 1201,...   \n",
       "3      [101, 138, 1532, 1104, 1112, 12866, 11990, 151...   \n",
       "4      [101, 1109, 1112, 12866, 11990, 12753, 117, 17...   \n",
       "...                                                  ...   \n",
       "46787  [101, 8652, 1116, 1943, 12556, 1968, 2822, 117...   \n",
       "46788  [101, 1220, 1309, 1737, 2310, 1106, 1129, 1625...   \n",
       "46789  [101, 1335, 1314, 1480, 112, 188, 11158, 117, ...   \n",
       "46790  [101, 169, 169, 1284, 19291, 9360, 1272, 1195,...   \n",
       "46791  [101, 169, 169, 1284, 1328, 1106, 1267, 5232, ...   \n",
       "\n",
       "                                                word_ids  \\\n",
       "0      [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...   \n",
       "1      [None, 0, 0, 1, 1, 2, 3, 4, 5, 5, 5, 6, 6, 6, ...   \n",
       "2      [None, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "3      [None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "4      [None, 0, 1, 1, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, ...   \n",
       "...                                                  ...   \n",
       "46787  [None, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, ...   \n",
       "46788            [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]   \n",
       "46789  [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11...   \n",
       "46790  [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...   \n",
       "46791  [None, 0, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10,...   \n",
       "\n",
       "                                                  labels  task_ids  \n",
       "0      [-100, 9, 9, -100, 39, 21, 0, 30, 39, 20, 10, ...         1  \n",
       "1      [-100, 9, -100, 9, -100, 33, 37, 5, 9, -100, -...         1  \n",
       "2      [-100, 9, 9, -100, -100, 39, 21, 0, 30, 19, 30...         1  \n",
       "3      [-100, 28, 37, 5, 37, -100, -100, 17, 26, 2, 1...         1  \n",
       "4      [-100, 28, 37, -100, -100, 37, 39, 37, -100, -...         1  \n",
       "...                                                  ...       ...  \n",
       "46787  [-100, 33, -100, 9, 9, -100, -100, 39, 37, 5, ...         1  \n",
       "46788    [-100, 43, 17, 27, 43, 2, 10, 37, 17, 18, -100]         1  \n",
       "46789  [-100, 5, 30, 37, 23, -100, 37, 39, 43, 27, 5,...         1  \n",
       "46790  [-100, 38, -100, 43, 16, 37, 5, 43, 16, 5, 28,...         1  \n",
       "46791  [-100, 38, -100, 43, 16, 2, 10, 9, 9, -100, 19...         1  \n",
       "\n",
       "[46792 rows x 6 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_task.train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "798f2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "# This class is adapted from: https://towardsdatascience.com/how-to-create-and-train-a-multi-task-transformer-model-18c54a146240\n",
    "class TokenClassificationModel(PreTrainedModel):    \n",
    "    def __init__(self, config, tasks):\n",
    "        super().__init__(config)\n",
    "        self.encoder = AutoModel.from_pretrained(transformer_name, config=config)\n",
    "\n",
    "        self.output_heads = nn.ModuleDict()\n",
    "        for task in tasks:\n",
    "            head = TokenClassificationHead(self.encoder.config.hidden_size, task.num_labels, config.hidden_dropout_prob)\n",
    "            # ModuleDict requires keys to be strings\n",
    "            self.output_heads[str(task.task_id)] = head\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for task_id in self.output_heads:\n",
    "            self.output_heads[task_id]._init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, task_ids=None, **kwargs):\n",
    "        outputs = self.encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            **kwargs,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        print(f'batch size = {len(input_ids)}')\n",
    "        print(f'task_ids in this batch: {task_ids}')\n",
    "        \n",
    "        unique_task_ids_list = torch.unique(task_ids).tolist()\n",
    "        for unique_task_id in unique_task_ids_list:\n",
    "            task_id_filter = task_ids == unique_task_id\n",
    "            filtered_sequence_output = sequence_output[task_id_filter]\n",
    "            print(f'size of batch for task {unique_task_id} is: {len(filtered_sequence_output)}')\n",
    "                    \n",
    "        logits = None\n",
    "        loss = None\n",
    "        # TODO: add heads here\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "class TokenClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if self.classifier.bias is not None:\n",
    "            self.classifier.bias.data.zero_()\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output, labels=None, attention_mask=None, **kwargs):\n",
    "        sequence_output_dropout = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output_dropout)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()            \n",
    "            inputs = logits.view(-1, self.num_labels)\n",
    "            targets = labels.view(-1)\n",
    "            loss = loss_fn(inputs, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c9945a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [ner_task, pos_task]\n",
    "config = AutoConfig.from_pretrained(transformer_name)\n",
    "model = TokenClassificationModel(config, tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "884e24a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=True,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=40,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=bert-base-cased-ner/runs/Sep30_14-55-13_dhcp-10-142-163-21.uawifi.arizona.edu,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=4,\n",
       "optim=adamw_hf,\n",
       "output_dir=bert-base-cased-ner,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=128,\n",
       "per_device_train_batch_size=128,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=bert-base-cased-ner,\n",
       "save_on_each_node=False,\n",
       "save_steps=500,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=True,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 4\n",
    "batch_size = 128\n",
    "weight_decay = 0.01\n",
    "model_name = f'{transformer_name}-ner'\n",
    "\n",
    "use_mps_device = True if str(device) == 'mps' else False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level='error',\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy='epoch',\n",
    "    weight_decay=weight_decay,\n",
    "    use_mps_device = use_mps_device\n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9c45587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # gold labels\n",
    "    label_ids = eval_pred.label_ids\n",
    "    # predictions\n",
    "    pred_ids = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    # collect gold and predicted labels, ignoring ignore_index label\n",
    "    y_true, y_pred = [], []\n",
    "    batch_size, seq_len = pred_ids.shape\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != ignore_index:\n",
    "                y_true.append(index_to_label[label_ids[i][j]])\n",
    "                y_pred.append(index_to_label[pred_ids[i][j]])\n",
    "    # return computed metrics\n",
    "    return {'accuracy': accuracy_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49167b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['words', 'str_labels', 'input_ids', 'word_ids', 'labels', 'task_ids', '__index_level_0__'],\n",
       "        num_rows: 61779\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['words', 'str_labels', 'input_ids', 'word_ids', 'labels', 'task_ids', '__index_level_0__'],\n",
       "        num_rows: 8504\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['words', 'str_labels', 'input_ids', 'word_ids', 'labels', 'task_ids', '__index_level_0__'],\n",
       "        num_rows: 6101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds['train'] = Dataset.from_pandas(pd.concat([ner_task.train_df, pos_task.train_df]))\n",
    "ds['validation'] = Dataset.from_pandas(pd.concat([ner_task.dev_df, pos_task.dev_df]))\n",
    "ds['test'] = Dataset.from_pandas(pd.concat([ner_task.test_df, pos_task.test_df]))\n",
    "\n",
    "# these are no longer needed; discard them to save memory\n",
    "ner_task.train_df = None\n",
    "ner_task.dev_df = None\n",
    "pos_task.train_df = None\n",
    "pos_task.dev_df = None\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4fd912e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 14:55:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size = 128\n",
      "task_ids in this batch: tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0], device='mps:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'println' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Time =\u001b[39m\u001b[38;5;124m\"\u001b[39m, now\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Time =\u001b[39m\u001b[38;5;124m\"\u001b[39m, now\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/transformers/trainer.py:1521\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1518\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1520\u001b[0m )\n\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/transformers/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1761\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1763\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1766\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1768\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1769\u001b[0m ):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/transformers/trainer.py:2499\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2498\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2499\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2502\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2530\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2531\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36mTokenClassificationModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, labels, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     task_id_filter \u001b[38;5;241m=\u001b[39m task_ids \u001b[38;5;241m==\u001b[39m unique_task_id\n\u001b[1;32m     38\u001b[0m     filtered_sequence_output \u001b[38;5;241m=\u001b[39m sequence_output[task_id_filter]\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mprintln\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize of batch for task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_task_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_sequence_output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'println' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datetime import datetime\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['validation'],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"Current Time =\", now.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"Current Time =\", now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4ddf0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.predict(ds['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd7bf945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-MISC', 'O', 'I-PER', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'B-LOC', 'I-ORG']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.82      0.84      0.83       702\n",
      "           O       1.00      0.99      1.00     38554\n",
      "       I-PER       0.99      0.98      0.99      1156\n",
      "       B-ORG       0.90      0.92      0.91      1661\n",
      "       B-PER       0.97      0.95      0.96      1617\n",
      "       I-LOC       0.87      0.92      0.90       257\n",
      "      I-MISC       0.65      0.78      0.71       216\n",
      "       B-LOC       0.94      0.94      0.94      1668\n",
      "       I-ORG       0.88      0.92      0.90       835\n",
      "\n",
      "    accuracy                           0.98     46666\n",
      "   macro avg       0.89      0.92      0.90     46666\n",
      "weighted avg       0.98      0.98      0.98     46666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "num_labels = model.num_labels\n",
    "label_ids = output.label_ids.reshape(-1)\n",
    "predictions = output.predictions.reshape(-1, num_labels)\n",
    "predictions = np.argmax(predictions, axis=-1)\n",
    "mask = label_ids != ignore_index\n",
    "\n",
    "y_true = label_ids[mask]\n",
    "y_pred = predictions[mask]\n",
    "target_names = [index_to_label.get(ele, \"\") for ele in range(num_labels)]\n",
    "print(target_names)\n",
    "\n",
    "report = classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=target_names\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6221528",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test_out.txt\", \"w\")\n",
    "for i in range(0, len(y_true)):\n",
    "    f.write(f\"X {index_to_label.get(y_true[i])} {index_to_label.get(y_pred[i])}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24770d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
