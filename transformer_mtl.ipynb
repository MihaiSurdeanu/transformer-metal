{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae55af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch \n",
    "from torch import optim \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif 'arm64' in platform.platform():\n",
    "    device = torch.device('mps') # 'mps'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'device: {device.type}') \n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# pytorch ignores this label in the loss\n",
    "ignore_index = -100\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# which transformer to use\n",
    "transformer_name = \"bert-base-cased\" # 'xlm-roberta-base' # 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4beaebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# map labels to the first token in each word\n",
    "def align_labels(word_ids, labels, label_to_index):\n",
    "    label_ids = []\n",
    "    previous_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None or word_id == previous_word_id:\n",
    "            # ignore if not a word or word id has already been seen\n",
    "            label_ids.append(ignore_index)\n",
    "        else:\n",
    "            # get label id for corresponding word\n",
    "            label_id = label_to_index[labels[word_id]]\n",
    "            label_ids.append(label_id)\n",
    "        # remember this word id\n",
    "        previous_word_id = word_id\n",
    "    \n",
    "    return label_ids\n",
    "            \n",
    "# build a set of labels in the dataset            \n",
    "def read_label_set(fn):\n",
    "    labels = set()\n",
    "    with open(fn) as f:\n",
    "        for index, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            if tokens != []:\n",
    "                label = tokens[-1]\n",
    "                labels.add(label)\n",
    "    return labels\n",
    "\n",
    "# converts a two-column file in the basic MTL format (\"word \\t label\") into a dataframe\n",
    "def read_dataframe(fn, label_to_index, task_id):\n",
    "    # now build the actual dataframe for this dataset\n",
    "    data = {'words': [], 'str_labels': [], 'input_ids': [], 'word_ids': [], 'labels': [], 'task_ids': []}\n",
    "    with open(fn) as f:\n",
    "        sent_words = []\n",
    "        sent_labels = [] \n",
    "        for index, line in tqdm(enumerate(f)):\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            if tokens == []:\n",
    "                data['words'].append(sent_words)\n",
    "                data['str_labels'].append(sent_labels)\n",
    "                \n",
    "                # tokenize each sentence\n",
    "                token_input = tokenizer(sent_words, is_split_into_words = True)  \n",
    "                token_ids = token_input['input_ids']\n",
    "                word_ids = token_input.word_ids(batch_index = 0)\n",
    "                \n",
    "                # map labels to the first token in each word\n",
    "                token_labels = align_labels(word_ids, sent_labels, label_to_index)\n",
    "                \n",
    "                data['input_ids'].append(token_ids)\n",
    "                data['word_ids'].append(word_ids)\n",
    "                data['labels'].append(token_labels)\n",
    "                data['task_ids'].append(task_id)\n",
    "                sent_words = []\n",
    "                sent_labels = [] \n",
    "            else:\n",
    "                sent_words.append(tokens[0])\n",
    "                sent_labels.append(tokens[1])\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b3e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task():\n",
    "    def __init__(self, task_id, train_file_name, dev_file_name, test_file_name):\n",
    "        self.task_id = task_id\n",
    "        # we need an index of labels first\n",
    "        self.labels = read_label_set(train_file_name)\n",
    "        self.index_to_label = {i:t for i,t in enumerate(self.labels)}\n",
    "        self.label_to_index = {t:i for i,t in enumerate(self.labels)}\n",
    "        self.num_labels = len(self.index_to_label)\n",
    "        # create data frames for the datasets\n",
    "        self.train_df = read_dataframe(train_file_name, self.label_to_index, self.task_id)\n",
    "        self.dev_df = read_dataframe(dev_file_name, self.label_to_index, self.task_id)\n",
    "        self.test_df = read_dataframe(test_file_name, self.label_to_index, self.task_id)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d43b086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fe152b1f4243c3b47d5c5d23d71a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f415047fdc4d87a5057c0937bf37e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ec9c36e0284163ae68fe2df8230f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c1f5b6c9f043349aa71ef9457d3359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8657632ab4495eb377173e9d87a17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a680e74e064cb49dd16722cc27cf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_task = Task(0, \"data/conll-ner/train.txt\", \"data/conll-ner/dev.txt\", \"data/conll-ner/test.txt\")\n",
    "pos_task = Task(1, \"data/pos/train.txt\", \"data/pos/dev.txt\", \"data/pos/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90478984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>str_labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>task_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[101, 118, 141, 9244, 9272, 12426, 1942, 118, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 0, None]</td>\n",
       "      <td>[-100, 6, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
       "      <td>[101, 7270, 22961, 1528, 1840, 1106, 21423, 14...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]</td>\n",
       "      <td>[-100, 0, 6, 8, 6, 6, 6, 8, 6, -100, 6, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td>[101, 1943, 14428, 102]</td>\n",
       "      <td>[None, 0, 1, None]</td>\n",
       "      <td>[-100, 5, 1, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "      <td>[101, 26660, 13329, 12649, 15928, 1820, 118, 4...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]</td>\n",
       "      <td>[-100, 2, -100, -100, -100, 6, -100, -100, -10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...</td>\n",
       "      <td>[101, 1109, 1735, 2827, 1163, 1113, 9170, 1122...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 6, 0, 7, 6, 6, 6, 6, 6, 6, 8, 6, 6, 6, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14982</th>\n",
       "      <td>[on, Friday, :]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[101, 1113, 5286, 131, 102]</td>\n",
       "      <td>[None, 0, 1, 2, None]</td>\n",
       "      <td>[-100, 6, 6, 6, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14983</th>\n",
       "      <td>[Division, two]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[101, 1784, 1160, 102]</td>\n",
       "      <td>[None, 0, 1, None]</td>\n",
       "      <td>[-100, 6, 6, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14984</th>\n",
       "      <td>[Plymouth, 2, Preston, 1]</td>\n",
       "      <td>[B-ORG, O, B-ORG, O]</td>\n",
       "      <td>[101, 10033, 123, 8083, 122, 102]</td>\n",
       "      <td>[None, 0, 1, 2, 3, None]</td>\n",
       "      <td>[-100, 0, 6, 0, 6, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>[Division, three]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[101, 1784, 1210, 102]</td>\n",
       "      <td>[None, 0, 1, None]</td>\n",
       "      <td>[-100, 6, 6, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14986</th>\n",
       "      <td>[Swansea, 1, Lincoln, 2]</td>\n",
       "      <td>[B-ORG, O, B-ORG, O]</td>\n",
       "      <td>[101, 17057, 122, 4617, 123, 102]</td>\n",
       "      <td>[None, 0, 1, 2, 3, None]</td>\n",
       "      <td>[-100, 0, 6, 0, 6, -100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14987 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   words  \\\n",
       "0                                           [-DOCSTART-]   \n",
       "1      [EU, rejects, German, call, to, boycott, Briti...   \n",
       "2                                     [Peter, Blackburn]   \n",
       "3                                 [BRUSSELS, 1996-08-22]   \n",
       "4      [The, European, Commission, said, on, Thursday...   \n",
       "...                                                  ...   \n",
       "14982                                    [on, Friday, :]   \n",
       "14983                                    [Division, two]   \n",
       "14984                          [Plymouth, 2, Preston, 1]   \n",
       "14985                                  [Division, three]   \n",
       "14986                           [Swansea, 1, Lincoln, 2]   \n",
       "\n",
       "                                              str_labels  \\\n",
       "0                                                    [O]   \n",
       "1              [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]   \n",
       "2                                         [B-PER, I-PER]   \n",
       "3                                             [B-LOC, O]   \n",
       "4      [O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...   \n",
       "...                                                  ...   \n",
       "14982                                          [O, O, O]   \n",
       "14983                                             [O, O]   \n",
       "14984                               [B-ORG, O, B-ORG, O]   \n",
       "14985                                             [O, O]   \n",
       "14986                               [B-ORG, O, B-ORG, O]   \n",
       "\n",
       "                                               input_ids  \\\n",
       "0      [101, 118, 141, 9244, 9272, 12426, 1942, 118, ...   \n",
       "1      [101, 7270, 22961, 1528, 1840, 1106, 21423, 14...   \n",
       "2                                [101, 1943, 14428, 102]   \n",
       "3      [101, 26660, 13329, 12649, 15928, 1820, 118, 4...   \n",
       "4      [101, 1109, 1735, 2827, 1163, 1113, 9170, 1122...   \n",
       "...                                                  ...   \n",
       "14982                        [101, 1113, 5286, 131, 102]   \n",
       "14983                             [101, 1784, 1160, 102]   \n",
       "14984                  [101, 10033, 123, 8083, 122, 102]   \n",
       "14985                             [101, 1784, 1210, 102]   \n",
       "14986                  [101, 17057, 122, 4617, 123, 102]   \n",
       "\n",
       "                                                word_ids  \\\n",
       "0                      [None, 0, 0, 0, 0, 0, 0, 0, None]   \n",
       "1             [None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]   \n",
       "2                                     [None, 0, 1, None]   \n",
       "3                [None, 0, 0, 0, 0, 1, 1, 1, 1, 1, None]   \n",
       "4      [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "...                                                  ...   \n",
       "14982                              [None, 0, 1, 2, None]   \n",
       "14983                                 [None, 0, 1, None]   \n",
       "14984                           [None, 0, 1, 2, 3, None]   \n",
       "14985                                 [None, 0, 1, None]   \n",
       "14986                           [None, 0, 1, 2, 3, None]   \n",
       "\n",
       "                                                  labels  task_ids  \n",
       "0      [-100, 6, -100, -100, -100, -100, -100, -100, ...         0  \n",
       "1          [-100, 0, 6, 8, 6, 6, 6, 8, 6, -100, 6, -100]         0  \n",
       "2                                     [-100, 5, 1, -100]         0  \n",
       "3      [-100, 2, -100, -100, -100, 6, -100, -100, -10...         0  \n",
       "4      [-100, 6, 0, 7, 6, 6, 6, 6, 6, 6, 8, 6, 6, 6, ...         0  \n",
       "...                                                  ...       ...  \n",
       "14982                              [-100, 6, 6, 6, -100]         0  \n",
       "14983                                 [-100, 6, 6, -100]         0  \n",
       "14984                           [-100, 0, 6, 0, 6, -100]         0  \n",
       "14985                                 [-100, 6, 6, -100]         0  \n",
       "14986                           [-100, 0, 6, 0, 6, -100]         0  \n",
       "\n",
       "[14987 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_task.train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a0c793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>str_labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>task_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "      <td>[101, 4855, 25354, 6378, 117, 5391, 1201, 1385...</td>\n",
       "      <td>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
       "      <td>[-100, 22, 22, -100, 26, 7, 29, 37, 26, 28, 6,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "      <td>[101, 1828, 119, 25354, 6378, 1110, 3931, 1104...</td>\n",
       "      <td>[None, 0, 0, 1, 1, 2, 3, 4, 5, 5, 5, 6, 6, 6, ...</td>\n",
       "      <td>[-100, 22, -100, 22, -100, 31, 2, 39, 22, -100...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "      <td>[101, 19922, 138, 8376, 2246, 117, 3731, 1201,...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 22, 22, -100, -100, 26, 7, 29, 37, 16, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "      <td>[101, 138, 1532, 1104, 1112, 12866, 11990, 151...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 42, 2, 39, 2, -100, -100, 33, 3, 30, 6,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
       "      <td>[101, 1109, 1112, 12866, 11990, 12753, 117, 17...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, ...</td>\n",
       "      <td>[-100, 42, 2, -100, -100, 2, 26, 2, -100, -100...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46787</th>\n",
       "      <td>[Says, Peter, Mokaba, ,, president, of, the, S...</td>\n",
       "      <td>[VBZ, NNP, NNP, ,, NN, IN, DT, NNP, NNP, NNP, ...</td>\n",
       "      <td>[101, 8652, 1116, 1943, 12556, 1968, 2822, 117...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, ...</td>\n",
       "      <td>[-100, 31, -100, 22, 22, -100, -100, 26, 2, 39...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46788</th>\n",
       "      <td>[They, never, considered, themselves, to, be, ...</td>\n",
       "      <td>[PRP, RB, VBD, PRP, TO, VB, NN, RB, .]</td>\n",
       "      <td>[101, 1220, 1309, 1737, 2310, 1106, 1129, 1625...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]</td>\n",
       "      <td>[-100, 38, 33, 19, 38, 30, 6, 2, 33, 1, -100]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46789</th>\n",
       "      <td>[At, last, night, 's, rally, ,, they, called, ...</td>\n",
       "      <td>[IN, JJ, NN, POS, NN, ,, PRP, VBD, IN, PRP$, N...</td>\n",
       "      <td>[101, 1335, 1314, 1480, 112, 188, 11158, 117, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
       "      <td>[-100, 39, 37, 2, 20, -100, 2, 26, 38, 19, 39,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46790</th>\n",
       "      <td>[``, We, emphasize, discipline, because, we, k...</td>\n",
       "      <td>[``, PRP, VBP, NN, IN, PRP, VBP, IN, DT, NN, V...</td>\n",
       "      <td>[101, 169, 169, 1284, 19291, 9360, 1272, 1195,...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
       "      <td>[-100, 21, -100, 38, 44, 2, 39, 38, 44, 39, 42...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46791</th>\n",
       "      <td>[``, We, want, to, see, Nelson, Mandela, and, ...</td>\n",
       "      <td>[``, PRP, VBP, TO, VB, NNP, NNP, CC, PDT, PRP$...</td>\n",
       "      <td>[101, 169, 169, 1284, 1328, 1106, 1267, 5232, ...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 21, -100, 38, 44, 30, 6, 22, 22, -100, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46792 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   words  \\\n",
       "0      [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1      [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
       "2      [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "3      [A, form, of, asbestos, once, used, to, make, ...   \n",
       "4      [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "...                                                  ...   \n",
       "46787  [Says, Peter, Mokaba, ,, president, of, the, S...   \n",
       "46788  [They, never, considered, themselves, to, be, ...   \n",
       "46789  [At, last, night, 's, rally, ,, they, called, ...   \n",
       "46790  [``, We, emphasize, discipline, because, we, k...   \n",
       "46791  [``, We, want, to, see, Nelson, Mandela, and, ...   \n",
       "\n",
       "                                              str_labels  \\\n",
       "0      [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...   \n",
       "1      [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...   \n",
       "2      [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...   \n",
       "3      [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...   \n",
       "4      [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...   \n",
       "...                                                  ...   \n",
       "46787  [VBZ, NNP, NNP, ,, NN, IN, DT, NNP, NNP, NNP, ...   \n",
       "46788             [PRP, RB, VBD, PRP, TO, VB, NN, RB, .]   \n",
       "46789  [IN, JJ, NN, POS, NN, ,, PRP, VBD, IN, PRP$, N...   \n",
       "46790  [``, PRP, VBP, NN, IN, PRP, VBP, IN, DT, NN, V...   \n",
       "46791  [``, PRP, VBP, TO, VB, NNP, NNP, CC, PDT, PRP$...   \n",
       "\n",
       "                                               input_ids  \\\n",
       "0      [101, 4855, 25354, 6378, 117, 5391, 1201, 1385...   \n",
       "1      [101, 1828, 119, 25354, 6378, 1110, 3931, 1104...   \n",
       "2      [101, 19922, 138, 8376, 2246, 117, 3731, 1201,...   \n",
       "3      [101, 138, 1532, 1104, 1112, 12866, 11990, 151...   \n",
       "4      [101, 1109, 1112, 12866, 11990, 12753, 117, 17...   \n",
       "...                                                  ...   \n",
       "46787  [101, 8652, 1116, 1943, 12556, 1968, 2822, 117...   \n",
       "46788  [101, 1220, 1309, 1737, 2310, 1106, 1129, 1625...   \n",
       "46789  [101, 1335, 1314, 1480, 112, 188, 11158, 117, ...   \n",
       "46790  [101, 169, 169, 1284, 19291, 9360, 1272, 1195,...   \n",
       "46791  [101, 169, 169, 1284, 1328, 1106, 1267, 5232, ...   \n",
       "\n",
       "                                                word_ids  \\\n",
       "0      [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...   \n",
       "1      [None, 0, 0, 1, 1, 2, 3, 4, 5, 5, 5, 6, 6, 6, ...   \n",
       "2      [None, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "3      [None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "4      [None, 0, 1, 1, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, ...   \n",
       "...                                                  ...   \n",
       "46787  [None, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, ...   \n",
       "46788            [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]   \n",
       "46789  [None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11...   \n",
       "46790  [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...   \n",
       "46791  [None, 0, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10,...   \n",
       "\n",
       "                                                  labels  task_ids  \n",
       "0      [-100, 22, 22, -100, 26, 7, 29, 37, 26, 28, 6,...         1  \n",
       "1      [-100, 22, -100, 22, -100, 31, 2, 39, 22, -100...         1  \n",
       "2      [-100, 22, 22, -100, -100, 26, 7, 29, 37, 16, ...         1  \n",
       "3      [-100, 42, 2, 39, 2, -100, -100, 33, 3, 30, 6,...         1  \n",
       "4      [-100, 42, 2, -100, -100, 2, 26, 2, -100, -100...         1  \n",
       "...                                                  ...       ...  \n",
       "46787  [-100, 31, -100, 22, 22, -100, -100, 26, 2, 39...         1  \n",
       "46788      [-100, 38, 33, 19, 38, 30, 6, 2, 33, 1, -100]         1  \n",
       "46789  [-100, 39, 37, 2, 20, -100, 2, 26, 38, 19, 39,...         1  \n",
       "46790  [-100, 21, -100, 38, 44, 2, 39, 38, 44, 39, 42...         1  \n",
       "46791  [-100, 21, -100, 38, 44, 30, 6, 22, 22, -100, ...         1  \n",
       "\n",
       "[46792 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_task.train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798f2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "# This class is adapted from: https://towardsdatascience.com/how-to-create-and-train-a-multi-task-transformer-model-18c54a146240\n",
    "class TokenClassificationModel(PreTrainedModel):    \n",
    "    def __init__(self, config, tasks):\n",
    "        super().__init__(config)\n",
    "        self.encoder = AutoModel.from_pretrained(transformer_name, config=config)\n",
    "\n",
    "        self.output_heads = nn.ModuleDict()\n",
    "        for task in tasks:\n",
    "            head = TokenClassificationHead(self.encoder.config.hidden_size, task.num_labels, config.hidden_dropout_prob)\n",
    "            # ModuleDict requires keys to be strings\n",
    "            self.output_heads[str(task.task_id)] = head\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for task_id in self.output_heads:\n",
    "            self.output_heads[task_id]._init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, task_ids=None, **kwargs):\n",
    "        outputs = self.encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            **kwargs,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        #print(f'batch size = {len(input_ids)}')\n",
    "        #print(f'task_ids in this batch: {task_ids}')\n",
    "        \n",
    "        # generate specific predictions and losses for each task head\n",
    "        unique_task_ids_list = torch.unique(task_ids).tolist()\n",
    "        logits = None\n",
    "        loss_list = []\n",
    "        for unique_task_id in unique_task_ids_list:\n",
    "            task_id_filter = task_ids == unique_task_id\n",
    "            filtered_sequence_output = sequence_output[task_id_filter]\n",
    "            filtered_labels = None if labels is None else labels[task_id_filter]\n",
    "            filtered_attention_mask = None if attention_mask is None else attention_mask[task_id_filter]\n",
    "            #print(f'size of batch for task {unique_task_id} is: {len(filtered_sequence_output)}')\n",
    "            logits, task_loss = self.output_heads[str(unique_task_id)].forward(\n",
    "                filtered_sequence_output, None,\n",
    "                filtered_labels,\n",
    "                filtered_attention_mask,\n",
    "            )\n",
    "            if filtered_labels is not None:\n",
    "                loss_list.append(task_loss)\n",
    "                \n",
    "        loss = None if len(loss_list) == 0 else torch.stack(loss_list)\n",
    "                    \n",
    "        # logits are only used for eval, in which case we handle a single task at a time\n",
    "        # TODO: allow all tasks in the forward pass at inference                     \n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss.mean(),\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "class TokenClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if self.classifier.bias is not None:\n",
    "            self.classifier.bias.data.zero_()\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output, labels=None, attention_mask=None, **kwargs):\n",
    "        sequence_output_dropout = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output_dropout)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()            \n",
    "            inputs = logits.view(-1, self.num_labels)\n",
    "            targets = labels.view(-1)\n",
    "            loss = loss_fn(inputs, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9945a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tasks = [ner_task, pos_task]\n",
    "config = AutoConfig.from_pretrained(transformer_name)\n",
    "model = TokenClassificationModel(config, tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c45587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # gold labels\n",
    "    label_ids = eval_pred.label_ids\n",
    "    # predictions\n",
    "    pred_ids = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    # collect gold and predicted labels, ignoring ignore_index label\n",
    "    y_true, y_pred = [], []\n",
    "    batch_size, seq_len = pred_ids.shape\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != ignore_index:\n",
    "                y_true.append(label_ids[i][j]) #index_to_label[label_ids[i][j]])\n",
    "                y_pred.append(pred_ids[i][j]) #index_to_label[pred_ids[i][j]])\n",
    "    # return computed metrics\n",
    "    return {'accuracy': accuracy_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49167b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['words', 'str_labels', 'input_ids', 'word_ids', 'labels', 'task_ids', '__index_level_0__'],\n",
       "        num_rows: 61779\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['words', 'str_labels', 'input_ids', 'word_ids', 'labels', 'task_ids', '__index_level_0__'],\n",
       "        num_rows: 8504\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['words', 'str_labels', 'input_ids', 'word_ids', 'labels', 'task_ids', '__index_level_0__'],\n",
       "        num_rows: 6101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds['train'] = Dataset.from_pandas(pd.concat([ner_task.train_df, pos_task.train_df]))\n",
    "ds['validation'] = Dataset.from_pandas(pd.concat([ner_task.dev_df, pos_task.dev_df]))\n",
    "ds['test'] = Dataset.from_pandas(pd.concat([ner_task.test_df, pos_task.test_df]))\n",
    "\n",
    "# these are no longer needed; discard them to save memory\n",
    "ner_task.train_df = None\n",
    "pos_task.train_df = None\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48554618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trainer, task, name):\n",
    "    print(f\"Evaluating on the validation dataset for task {name}:\")\n",
    "    ds = Dataset.from_pandas(task.dev_df)\n",
    "    scores = trainer.evaluate(ds)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4fd912e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1727.4766, 'train_samples_per_second': 35.763, 'train_steps_per_second': 0.28, 'train_loss': 0.05119461549241597, 'epoch': 1.0}\n",
      "Elapsed time for epoch 1: 0:28:47.460643\n",
      "Evaluating on the validation dataset for task NER:\n",
      "{'eval_loss': 0.010500318370759487, 'eval_runtime': 13.3186, 'eval_samples_per_second': 260.237, 'eval_steps_per_second': 2.102, 'epoch': 1.0}\n",
      "Evaluating on the validation dataset for task POS:\n",
      "{'eval_loss': 0.021637972444295883, 'eval_runtime': 21.634, 'eval_samples_per_second': 232.874, 'eval_steps_per_second': 1.849, 'epoch': 1.0}\n",
      "DEV MACRO LOSS FOR EPOCH 1: 0.016069145407527685\n",
      "\n",
      "\n",
      "STARTING EPOCH 2\n",
      "Resuming from checkpoint bert-base-cased-mtl/mtl_model_epoch1\n",
      "{'train_runtime': 1609.6763, 'train_samples_per_second': 38.38, 'train_steps_per_second': 0.3, 'train_loss': 0.01529753923909758, 'epoch': 1.0}\n",
      "Elapsed time for epoch 2: 0:26:49.661359\n",
      "Evaluating on the validation dataset for task NER:\n",
      "{'eval_loss': 0.008160301484167576, 'eval_runtime': 15.0045, 'eval_samples_per_second': 230.997, 'eval_steps_per_second': 1.866, 'epoch': 1.0}\n",
      "Evaluating on the validation dataset for task POS:\n",
      "{'eval_loss': 0.01663830131292343, 'eval_runtime': 26.2024, 'eval_samples_per_second': 192.272, 'eval_steps_per_second': 1.527, 'epoch': 1.0}\n",
      "DEV MACRO LOSS FOR EPOCH 2: 0.012399301398545504\n",
      "\n",
      "\n",
      "STARTING EPOCH 3\n",
      "Resuming from checkpoint bert-base-cased-mtl/mtl_model_epoch2\n",
      "{'train_runtime': 1649.707, 'train_samples_per_second': 37.448, 'train_steps_per_second': 0.293, 'train_loss': 0.012021477424826928, 'epoch': 1.0}\n",
      "Elapsed time for epoch 3: 0:27:29.691661\n",
      "Evaluating on the validation dataset for task NER:\n",
      "{'eval_loss': 0.008295220322906971, 'eval_runtime': 13.358, 'eval_samples_per_second': 259.47, 'eval_steps_per_second': 2.096, 'epoch': 1.0}\n",
      "Evaluating on the validation dataset for task POS:\n",
      "{'eval_loss': 0.01434375811368227, 'eval_runtime': 22.0559, 'eval_samples_per_second': 228.42, 'eval_steps_per_second': 1.814, 'epoch': 1.0}\n",
      "DEV MACRO LOSS FOR EPOCH 3: 0.01131948921829462\n",
      "\n",
      "\n",
      "STARTING EPOCH 4\n",
      "Resuming from checkpoint bert-base-cased-mtl/mtl_model_epoch3\n",
      "{'train_runtime': 1656.8664, 'train_samples_per_second': 37.287, 'train_steps_per_second': 0.292, 'train_loss': 0.010244144416003494, 'epoch': 1.0}\n",
      "Elapsed time for epoch 4: 0:27:36.864604\n",
      "Evaluating on the validation dataset for task NER:\n",
      "{'eval_loss': 0.008552570827305317, 'eval_runtime': 13.437, 'eval_samples_per_second': 257.944, 'eval_steps_per_second': 2.084, 'epoch': 1.0}\n",
      "Evaluating on the validation dataset for task POS:\n",
      "{'eval_loss': 0.012782610021531582, 'eval_runtime': 93.7024, 'eval_samples_per_second': 53.766, 'eval_steps_per_second': 0.427, 'epoch': 1.0}\n",
      "DEV MACRO LOSS FOR EPOCH 4: 0.01066759042441845\n",
      "\n",
      "\n",
      "STARTING EPOCH 5\n",
      "Resuming from checkpoint bert-base-cased-mtl/mtl_model_epoch4\n",
      "{'train_runtime': 4302.7759, 'train_samples_per_second': 14.358, 'train_steps_per_second': 0.112, 'train_loss': 0.008990949231892146, 'epoch': 1.0}\n",
      "Elapsed time for epoch 5: 0:25:51.558170\n",
      "Evaluating on the validation dataset for task NER:\n",
      "{'eval_loss': 0.008642928674817085, 'eval_runtime': 12.8912, 'eval_samples_per_second': 268.865, 'eval_steps_per_second': 2.172, 'epoch': 1.0}\n",
      "Evaluating on the validation dataset for task POS:\n",
      "{'eval_loss': 0.011404762044548988, 'eval_runtime': 23.7143, 'eval_samples_per_second': 212.446, 'eval_steps_per_second': 1.687, 'epoch': 1.0}\n",
      "DEV MACRO LOSS FOR EPOCH 5: 0.010023845359683037\n",
      "\n",
      "\n",
      "STARTING EPOCH 6\n",
      "Resuming from checkpoint bert-base-cased-mtl/mtl_model_epoch5\n",
      "{'train_runtime': 1520.7112, 'train_samples_per_second': 40.625, 'train_steps_per_second': 0.318, 'train_loss': 0.007863239718766932, 'epoch': 1.0}\n",
      "Elapsed time for epoch 6: 0:25:20.670869\n",
      "Evaluating on the validation dataset for task NER:\n",
      "{'eval_loss': 0.009036046452820301, 'eval_runtime': 13.1886, 'eval_samples_per_second': 262.803, 'eval_steps_per_second': 2.123, 'epoch': 1.0}\n",
      "Evaluating on the validation dataset for task POS:\n",
      "{'eval_loss': 0.010213040746748447, 'eval_runtime': 24.3822, 'eval_samples_per_second': 206.626, 'eval_steps_per_second': 1.641, 'epoch': 1.0}\n",
      "DEV MACRO LOSS FOR EPOCH 6: 0.009624543599784374\n",
      "\n",
      "\n",
      "STARTING EPOCH 7\n",
      "Resuming from checkpoint bert-base-cased-mtl/mtl_model_epoch6\n",
      "{'train_runtime': 1523.7126, 'train_samples_per_second': 40.545, 'train_steps_per_second': 0.317, 'train_loss': 0.00719013372069807, 'epoch': 1.0}\n",
      "Elapsed time for epoch 7: 0:25:23.713126\n",
      "Evaluating on the validation dataset for task NER:\n",
      "{'eval_loss': 0.009287795051932335, 'eval_runtime': 14.932, 'eval_samples_per_second': 232.119, 'eval_steps_per_second': 1.875, 'epoch': 1.0}\n",
      "Evaluating on the validation dataset for task POS:\n",
      "{'eval_loss': 0.009281126782298088, 'eval_runtime': 26.8193, 'eval_samples_per_second': 187.85, 'eval_steps_per_second': 1.491, 'epoch': 1.0}\n",
      "DEV MACRO LOSS FOR EPOCH 7: 0.009284460917115211\n",
      "\n",
      "\n",
      "STARTING EPOCH 8\n",
      "Resuming from checkpoint bert-base-cased-mtl/mtl_model_epoch7\n",
      "{'train_runtime': 1609.4962, 'train_samples_per_second': 38.384, 'train_steps_per_second': 0.3, 'train_loss': 0.006468432290213448, 'epoch': 1.0}\n",
      "Elapsed time for epoch 8: 0:26:49.504225\n",
      "Evaluating on the validation dataset for task NER:\n",
      "{'eval_loss': 0.009731603786349297, 'eval_runtime': 14.5994, 'eval_samples_per_second': 237.408, 'eval_steps_per_second': 1.918, 'epoch': 1.0}\n",
      "Evaluating on the validation dataset for task POS:\n",
      "{'eval_loss': 0.008418391458690166, 'eval_runtime': 24.1912, 'eval_samples_per_second': 208.258, 'eval_steps_per_second': 1.653, 'epoch': 1.0}\n",
      "DEV MACRO LOSS FOR EPOCH 8: 0.009074997622519732\n",
      "\n",
      "\n",
      "STARTING EPOCH 9\n",
      "Resuming from checkpoint bert-base-cased-mtl/mtl_model_epoch8\n",
      "{'train_runtime': 1612.0822, 'train_samples_per_second': 38.322, 'train_steps_per_second': 0.3, 'train_loss': 0.00582544996131281, 'epoch': 1.0}\n",
      "Elapsed time for epoch 9: 0:26:52.099445\n",
      "Evaluating on the validation dataset for task NER:\n",
      "{'eval_loss': 0.009499219246208668, 'eval_runtime': 14.2938, 'eval_samples_per_second': 242.483, 'eval_steps_per_second': 1.959, 'epoch': 1.0}\n",
      "Evaluating on the validation dataset for task POS:\n",
      "{'eval_loss': 0.007576359901577234, 'eval_runtime': 25.5201, 'eval_samples_per_second': 197.413, 'eval_steps_per_second': 1.567, 'epoch': 1.0}\n",
      "DEV MACRO LOSS FOR EPOCH 9: 0.008537789573892951\n",
      "\n",
      "\n",
      "STARTING EPOCH 10\n",
      "Resuming from checkpoint bert-base-cased-mtl/mtl_model_epoch9\n",
      "{'train_runtime': 1581.1747, 'train_samples_per_second': 39.072, 'train_steps_per_second': 0.305, 'train_loss': 0.005256790058459801, 'epoch': 1.0}\n",
      "Elapsed time for epoch 10: 0:26:21.181930\n",
      "Evaluating on the validation dataset for task NER:\n",
      "{'eval_loss': 0.010062891989946365, 'eval_runtime': 14.6881, 'eval_samples_per_second': 235.973, 'eval_steps_per_second': 1.906, 'epoch': 1.0}\n",
      "Evaluating on the validation dataset for task POS:\n",
      "{'eval_loss': 0.007180959451943636, 'eval_runtime': 24.5041, 'eval_samples_per_second': 205.598, 'eval_steps_per_second': 1.632, 'epoch': 1.0}\n",
      "DEV MACRO LOSS FOR EPOCH 10: 0.008621925720945\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "weight_decay = 0.01\n",
    "use_mps_device = True if str(device) == 'mps' else False\n",
    "model_name = f'{transformer_name}-mtl'\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "last_checkpoint = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f'STARTING EPOCH {epoch}')\n",
    "    if last_checkpoint != None:\n",
    "        print(f'Resuming from checkpoint {last_checkpoint}')\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_name,\n",
    "        log_level='error',\n",
    "        num_train_epochs=1, # one epoch at a time\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        # evaluation_strategy='epoch',\n",
    "        do_eval=False, # we will evaluate each task explicitly\n",
    "        weight_decay=weight_decay,\n",
    "        resume_from_checkpoint = last_checkpoint,\n",
    "        use_mps_device = use_mps_device\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        # compute_metrics=compute_metrics,\n",
    "        train_dataset=ds['train'],\n",
    "        # eval_dataset=ds['validation'],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "    trainer.train()\n",
    "    end_time = time.monotonic()\n",
    "    print(f\"Elapsed time for epoch {epoch}: {timedelta(seconds=end_time - start_time)}\")\n",
    "\n",
    "    ner_scores = evaluate(trainer, ner_task, \"NER\")\n",
    "    pos_scores = evaluate(trainer, pos_task, \"POS\")\n",
    "    macro_loss = (ner_scores['eval_loss'] + pos_scores['eval_loss'])/2\n",
    "    print(f'DEV MACRO LOSS FOR EPOCH {epoch}: {macro_loss}\\n\\n')\n",
    "\n",
    "    last_checkpoint = training_args.output_dir + '/mtl_model_epoch' + str(epoch)\n",
    "    trainer.save_model(last_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba0ddaf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'from_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTokenClassificationModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-cased-mtl/mtl_model_epoch9\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/transformers/modeling_utils.py:1918\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   1917\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 1918\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[1;32m   1919\u001b[0m         config_path,\n\u001b[1;32m   1920\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1921\u001b[0m         return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1922\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1923\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m   1924\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1925\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1926\u001b[0m         use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[1;32m   1927\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1928\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   1929\u001b[0m         _from_auto\u001b[38;5;241m=\u001b[39mfrom_auto_class,\n\u001b[1;32m   1930\u001b[0m         _from_pipeline\u001b[38;5;241m=\u001b[39mfrom_pipeline,\n\u001b[1;32m   1931\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1932\u001b[0m     )\n\u001b[1;32m   1933\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1934\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'from_pretrained'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "pred_config = AutoConfig.from_pretrained(transformer_name)\n",
    "model = TokenClassificationModel.from_pretrained('bert-base-cased-mtl/mtl_model_epoch9', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f3a1a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def classification_report_on_test(trainer, task, name):\n",
    "    print(f\"Test classification report for task {name}:\")\n",
    "    num_labels = task.num_labels\n",
    "    ds = Dataset.from_pandas(task.test_df)\n",
    "    output = trainer.predict(ds)\n",
    "    label_ids = output.label_ids.reshape(-1)\n",
    "    predictions = output.predictions.reshape(-1, num_labels)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    mask = label_ids != ignore_index\n",
    "    \n",
    "    y_true = label_ids[mask]\n",
    "    y_pred = predictions[mask]\n",
    "    target_names = [task.index_to_label.get(ele, \"\") for ele in range(num_labels)]\n",
    "    print(target_names)\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for(t, p) in zip(y_true, y_pred):\n",
    "        total = total + 1\n",
    "        if t == p:\n",
    "            correct = correct + 1\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=target_names\n",
    "    )\n",
    "    print(report)\n",
    "    print(f'locally computed accuracy: {accuracy}')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "864a1bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification report for task NER:\n",
      "['B-PER', 'B-LOC', 'O', 'I-ORG', 'B-MISC', 'I-MISC', 'B-ORG', 'I-PER', 'I-LOC']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-PER       0.96      0.95      0.96      1617\n",
      "       B-LOC       0.92      0.92      0.92      1668\n",
      "           O       1.00      0.99      0.99     38554\n",
      "       I-ORG       0.86      0.89      0.87       835\n",
      "      B-MISC       0.76      0.82      0.79       702\n",
      "      I-MISC       0.63      0.73      0.68       216\n",
      "       B-ORG       0.89      0.88      0.89      1661\n",
      "       I-PER       0.98      0.99      0.99      1156\n",
      "       I-LOC       0.80      0.87      0.83       257\n",
      "\n",
      "    accuracy                           0.98     46666\n",
      "   macro avg       0.87      0.89      0.88     46666\n",
      "weighted avg       0.98      0.98      0.98     46666\n",
      "\n",
      "locally computed accuracy: 0.9784211203017186\n",
      "Test classification report for task POS:\n",
      "['FW', 'SYM', 'NNP', 'NN', 'VBN', 'JJ', ':', \"''\", '#', ',', 'VBG', 'IN', 'RP', 'PDT', 'NNPS', '.', '``', 'RBS', '-RRB-', 'POS', '-LRB-', 'CC', 'WP$', 'NNS', 'VBZ', 'MD', 'RB', 'PRP', 'RBR', 'CD', 'VB', 'TO', 'VBP', 'WP', 'DT', 'WDT', 'JJS', 'WRB', 'UH', 'LS', '$', 'EX', 'JJR', 'PRP$', 'VBD']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FW       0.00      0.00      0.00        20\n",
      "         SYM       0.00      0.00      0.00         1\n",
      "         NNP       0.99      0.96      0.97      5918\n",
      "          NN       0.98      0.97      0.98      7568\n",
      "         VBN       0.85      0.98      0.91      1099\n",
      "          JJ       0.96      0.91      0.93      3707\n",
      "           :       1.00      1.00      1.00       241\n",
      "          ''       1.00      1.00      1.00       512\n",
      "           #       1.00      1.00      1.00         5\n",
      "           ,       1.00      1.00      1.00      3061\n",
      "         VBG       0.92      0.98      0.95       817\n",
      "          IN       0.98      0.99      0.99      5854\n",
      "          RP       0.86      0.90      0.88       182\n",
      "         PDT       0.71      0.28      0.40        18\n",
      "        NNPS       0.15      0.71      0.24        42\n",
      "           .       1.00      1.00      1.00      2363\n",
      "          ``       1.00      1.00      1.00       531\n",
      "         RBS       0.90      0.90      0.90        30\n",
      "       -RRB-       1.00      1.00      1.00        72\n",
      "         POS       0.98      0.99      0.99       549\n",
      "       -LRB-       0.99      1.00      0.99        72\n",
      "          CC       0.99      0.99      0.99      1373\n",
      "         WP$       1.00      1.00      1.00        21\n",
      "         NNS       0.99      0.99      0.99      3507\n",
      "         VBZ       0.98      0.99      0.99      1233\n",
      "          MD       1.00      1.00      1.00       584\n",
      "          RB       0.97      0.91      0.94      2053\n",
      "         PRP       1.00      1.00      1.00      1055\n",
      "         RBR       0.93      0.88      0.90       112\n",
      "          CD       0.98      1.00      0.99      1924\n",
      "          VB       0.98      0.98      0.98      1580\n",
      "          TO       1.00      1.00      1.00      1240\n",
      "         VBP       0.96      0.99      0.98       802\n",
      "          WP       0.98      1.00      0.99       111\n",
      "          DT       0.99      1.00      1.00      4816\n",
      "         WDT       0.99      0.97      0.98       280\n",
      "         JJS       0.97      0.97      0.97       127\n",
      "         WRB       1.00      0.99      1.00       133\n",
      "          UH       0.00      0.00      0.00         8\n",
      "          LS       0.00      0.00      0.00         4\n",
      "           $       1.00      1.00      1.00       375\n",
      "          EX       0.95      1.00      0.97        57\n",
      "         JJR       0.93      0.96      0.95       198\n",
      "        PRP$       0.99      1.00      1.00       509\n",
      "         VBD       0.99      0.97      0.98      1836\n",
      "\n",
      "    accuracy                           0.98     56600\n",
      "   macro avg       0.86      0.87      0.86     56600\n",
      "weighted avg       0.98      0.98      0.98     56600\n",
      "\n",
      "locally computed accuracy: 0.9768021201413427\n",
      "MTL macro accuracy: 0.9776116202215306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/msurdeanu/miniconda3/envs/transformers-mps/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "ner_acc = classification_report_on_test(trainer, ner_task, \"NER\")\n",
    "pos_acc = classification_report_on_test(trainer, pos_task, \"POS\")\n",
    "macro_acc = (ner_acc + pos_acc)/2\n",
    "print(f\"MTL macro accuracy: {macro_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24770d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
